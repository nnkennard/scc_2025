Published as a conference paper at ICLR 2018 ZEROSHOT VISUAL IMITATION Deepak Pathak∗, Parsa Mahmoudieh∗, Guanghao Luo∗, Pulkit Agrawal∗, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A. Efros, Trevor Darrell UC Berkeley { pathak,parsa. m,michaelluo,pulkitag,dianchen, fredshentu,shelhamer, malik,efros,trevor}@cs. berkeley. edu ABSTRACT The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both what and how to imitate. We pursue an alternative paradigm wherein an agent ﬁrst explores the world without any expert supervision and then distills its experience into a goalconditioned skill policy with a novel forward consistency loss. In our framework, the role of the expert is only to communicate the goals ( i.e., what to imitate) during inference. The learned policy is then employed to mimic the expert ( i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task. Our method is “zero-shot” in the sense that the agent never has access to expert actions during training or for the task demonstration at inference. We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen ofﬁce environments with a TurtleBot. Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance. Videos, models, and more details are available at https: //pathak22. github. io/zeroshot-imitation/. 1 INTRODUCTION Imitating expert demonstration is a powerful mechanism for learning to perform tasks from raw sensory observations. The current dominant paradigm in learning from demonstration ( LfD) (Argall et al. , 2009; Ng & Russell, 2000; Pomerleau, 1989; Schaal, 1999) requires the expert to either manually move the robot joints ( i.e., kinesthetic teaching) or teleoperate the robot to execute the desired task. The expert typically provides multiple demonstrations of a task at training time, and this generates data in the form of observationaction pairs from the agent’s point of view. The agent then distills this data into a policy for performing the task of interest. Such a heavily supervised approach, where it is necessary to provide demonstrations by controlling the robot, is incredibly tedious for the human expert. Moreover, for every new task that the robot needs to execute, the expert is required to provide a new set of demonstrations. Instead of communicating how to perform a task via observationaction pairs, a more general formulation allows the expert to communicate only what needs to be done by providing the observations of the desired world states via a video or a sparse sequence of images. This way, the agent is required to infer how to perform the task ( i.e., actions) by itself. In psychology, this is known as observational learning ( Bandura & Walters, 1977). While this is a harder learning problem, it is a more interesting setting, because the expert can demonstrate multiple tasks quickly and easily. An agent without any prior knowledge will ﬁnd it extremely hard to imitate a task by simply watching a visual demonstration in all but the simplest of cases. Thus, the natural question is: in order to imitate, what form of prior knowledge must the agent possess? A large body of work (Breazeal & Scassellati, 2002; Dillmann, 2004; Ikeuchi & Suehiro, 1994; Kuniyoshi et al. , 1989; 1994; Yang et al. , 2015) has sought to capture prior knowledge by manually predeﬁning the state that must be ∗Denotes equal contribution. 1

Published as a conference paper at ICLR 2018

L ( at < l a t e x i t s h a 1 _ b a s e 6 4 = " b M t g 3 E O Q o R 3 k j b w 2 v e r d 4 2 r y p I M = " > A A A C A n i c b V B N S 8 N A E N 3 U r 1 q / q t 7 0 E i x C B S l J E d R b w Y s H D x W M L T Q h T L b b d u n m g 9 2 J U E L B i 3 / F i w c V r / 4 K b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w h Z b 1 b R S W l l d W 1 4 r r p Y 3 N r e 2 d 8 u 7 e v Y p T S Z l D Y x H L d g C K C R 4 x B z k K 1 k 4 k g z A Q r B U M r y Z + 6 4 F J x e P o D k c J 8 0 L o R 7 z H K a C W / P K B G w I O K I j s Z l w F H 0 / d A W A G Y x 9 P / H L F q l l T m I v E z k m F 5 G j 6 5 S + 3 G 9 M 0 Z B F S A U p 1 b C t B L w O J n A o 2 L r m p Y g n Q I f R Z R 9 M I Q q a 8 b P r D 2 D z W S t f s x V J X h O Z U / T 2 R Q a j U K A x 0 5 + R i N e 9 N x P + 8 T o q 9 C y / j U Z I i i + h s U S 8 V J s b m J B C z y y W j K E a a A J V c 3 2 r S A U i g q G M r 6 R D s + Z c X i V O v X d b s 2 7 N K o 5 6 n U S S H 5 I h U i U 3 O S Y N c k y Z x C C W P 5 J m 8 k j f j y X g x 3 o 2 P W W v B y G f 2 y R 8 Y n z 8 0 p p d j < / l a t e x i t >

,

aˆt)

aˆ t < l a t e x i t s h a 1 _ b a s e 6 4 = " 8 P q 4 / T 3 q 7 j y S B K y / b F 5 s P U I 8 V f 4 = " > A A A B 7 3 i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 4 K X j x W M L b S h j L Z b t q l m 0 3 Y n Q g l 9 F d 4 8 a D i 1 b / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E p h 0 H W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x 6 M E m m G f d Z I h P d C c F w K R T 3 U a D k n V R z i E P J 2 + H 4 Z u a 3 n 7 g 2 I l H 3 O E l 5 E M N Q i U g w Q C s 9 9 k a A O U z 7 2 K / W 3 L o 7 B 1 0 l X k F q p E C r X / 3 q D R K W x V w h k 2 B M 1 3 N T D H L Q K J j k 0 0 o v M z w F N o Y h 7 1 q q I O Y m y O c H T + m Z V Q Y 0 S r Q t h X S u / p 7 I I T Z m E o e 2 M w Y c m W V v J v 7 n d T O M r o J c q D R D r t h i U Z R J i g m d f U 8 H Q n O G c m I J M C 3 s r Z S N Q A N D m 1 H F h u A t v 7 x K / I v 6 d d 2 7 a 9 S a j S K N M j k h p + S c e O S S N M k t a R G f M B K T Z / J K 3 h z t v D j v z s e i t e Q U M 8 f k D 5 z P H 4 Q k k F 8 = < / l a t e x i t >

feed-forward < l a t e x i t s h a 1 _ b a s e 6 4 = " A W l 8 5 E u k k 0 w 0 X t y q h X M H P z 8 x 0 K s = " > A A A B 8 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B i y X p R b 0 V v H i s Y G y h D W W z m b R L N 5 u w u 1 F K 6 d / w 4 k H F q 7 / G m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M B N c G 9 f 9 d k p r 6 x u b W + X t y s 7 u 3 v 5 B 9 f D o Q a e 5 Y u i z V K S q E 1 K N g k v 0 D T c C O 5 l C m o Q C 2 + H o Z u a 3 H 1 F p n s p 7 M 8 4 w S O h A 8 p g z a q z U i x G j i z h V T 1 R F / W r N r b t z k F X i F a Q G B V r 9 6 l c v S l m e o D R M U K 2 7 n p u Z Y E K V 4 U z g t N L L N W a U j e g A u 5 Z K m q A O J v O b p + T M K h G x q 2 1 J Q + b q 7 4 k J T b Q e J 6 H t T K g Z 6 m V v J v 7 n d X M T X w U T L r P c o G S L R X E u i E n J L A A S c Y X M i L E l l C l u b y V s S B V l x s Z U s S F 4 y y + v E r 9 R v 6 5 7 d 4 1 a s 1 G k U Y Y T O I V z 8 O A S m n A L L f C B Q Q b P 8 A p v T u 6 8 O O / O x 6 K 1 5 B Q z x / A H z u c P a L y R a g = = < / l a t e x i t >

L ( at < l a t e x i t s h a 1 _ b a s e 6 4 = " b M t g 3 E O Q o R 3 k j b w 2 v e r d 4 2 r y p I M = " > A A A C A n i c b V B N S 8 N A E N 3 U r 1 q / q t 7 0 E i x C B S l J E d R b w Y s H D x W M L T Q h T L b b d u n m g 9 2 J U E L B i 3 / F i w c V r / 4 K b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w h Z b 1 b R S W l l d W 1 4 r r p Y 3 N r e 2 d 8 u 7 e v Y p T S Z l D Y x H L d g C K C R 4 x B z k K 1 k 4 k g z A Q r B U M r y Z + 6 4 F J x e P o D k c J 8 0 L o R 7 z H K a C W / P K B G w I O K I j s Z l w F H 0 / d A W A G Y x 9 P / H L F q l l T m I v E z k m F 5 G j 6 5 S + 3 G 9 M 0 Z B F S A U p 1 b C t B L w O J n A o 2 L r m p Y g n Q I f R Z R 9 M I Q q a 8 b P r D 2 D z W S t f s x V J X h O Z U / T 2 R Q a j U K A x 0 5 + R i N e 9 N x P + 8 T o q 9 C y / j U Z I i i + h s U S 8 V J s b m J B C z y y W j K E a a A J V c 3 2 r S A U i g q G M r 6 R D s + Z c X i V O v X d b s 2 7 N K o 5 6 n U S S H 5 I h U i U 3 O S Y N c k y Z x C C W P 5 J m 8 k j f j y X g x 3 o 2 P W W v B y G f 2 y R 8 Y n z 8 0 p p d j < / l a t e x i t >

,

aˆt)

aˆ t < l a t e x i t s h a 1 _ b a s e 6 4 = " 8 P q 4 / T 3 q 7 j y S B K y / b F 5 s P U I 8 V f 4 = " > A A A B 7 3 i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 4 K X j x W M L b S h j L Z b t q l m 0 3 Y n Q g l 9 F d 4 8 a D i 1 b / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E p h 0 H W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x 6 M E m m G f d Z I h P d C c F w K R T 3 U a D k n V R z i E P J 2 + H 4 Z u a 3 n 7 g 2 I l H 3 O E l 5 E M N Q i U g w Q C s 9 9 k a A O U z 7 2 K / W 3 L o 7 B 1 0 l X k F q p E C r X / 3 q D R K W x V w h k 2 B M 1 3 N T D H L Q K J j k 0 0 o v M z w F N o Y h 7 1 q q I O Y m y O c H T + m Z V Q Y 0 S r Q t h X S u / p 7 I I T Z m E o e 2 M w Y c m W V v J v 7 n d T O M r o J c q D R D r t h i U Z R J i g m d f U 8 H Q n O G c m I J M C 3 s r Z S N Q A N D m 1 H F h u A t v 7 x K / I v 6 d d 2 7 a 9 S a j S K N M j k h p + S c e O S S N M k t a R G f M B K T Z / J K 3 h z t v D j v z s e i t e Q U M 8 f k D 5 z P H 4 Q k k F 8 = < / l a t e x i t > recurrent st at e < l a t e x i t s h a 1 _ b a s e 6 4 = " w 0 N 0 c O T T 7 j 6 q o j Q o l L X R B + a C d Y g = " > A A A C F X i c b V B P S 8 M w H E 3 n v 1 n / V T 1 6 C Q 7 B i 6 P d R b 0 N v H i c 4 N x g L S N N f 9 3 C 0 r Q k q T D K P o U X v 4 o X D y p e B W 9 + G 7 O t g m 4 + C L y 8 9 3 s k v x d m n C n t u l 9 W Z W V 1 b X 2 j u m l v b e / s 7 j n 7 B 3 c q z S W F N k 1 5 K r s h U c C Z g L Z m m k M 3 k 0 C S k E M n H F 1 N / c 4 9 S M V S c a v H G Q Q J G Q g W M 0 q 0 k f r O m R / C g I m C g t A g J 7 Y E m k t p L r 5 v K 0 0 0 2 D 6 I 6 M f u O z W 3 7 s 6 A l 4 l X k h o q 0 e o 7 n 3 6 U 0 j w x c c q J U j 3 P z X R Q E K k Z 5 T C x / V x B R u i I D K B n q C A J q K C Y r T X B J 0 a J c J x K c 4 T G M / V 3 o i C J U u M k N J M J 0 U O 1 6 E 3 F / 7 x e r u O L o G A i y z U I O n 8 o z j n W K Z 5 2 h C N m a t B 8 b A i h k p m / Y j o k k l D T g b J N C d 7 i y s u k 3 a h f 1 r 2 b R q 3 Z K N u o o i N 0 j E 6 R h 8 5 R E 1 2 j F m o j i h 7 Q E 3 p B r 9 a j 9 W y 9 W e / z 0 Y p V Z g 7 R H 1 g f 3 8 K T n + Y = < / l a t e x i t >

L ( x t+1 < l a t e x i t s h a 1 _ b a s e 6 4 = " S D j S A K s S x y 5 T x n 0 e E G x K C s 9 W e 1 0 = " > A A A C C n i c b V D L S s N A F J 3 U V 6 2 v q E s 3 o U W o K C U p g r o r u H H h o o K x h S a E y X T a D p 0 8 m L m R l p C 9 G 3 / F j Q s V t 3 6 B O / / G S Z u F V g 8 M n D n n X u 6 9 x 4 8 5 k 2 C a X 1 p p a X l l d a 2 8 X t n Y 3 N r e 0 X f 3 7 m S U C E J t E v F I d H 0 s K W c h t Y E B p 9 1 Y U B z 4 n H b 8 8 W X u d + 6 p k C w K b 2 E a U z f A w 5 A N G M G g J E + v O g G G E c E 8 v c 7 q E y + F Y y s 7 c U Y Y 0 k k 2 / x 1 5 e s 1 s m D M Y f 4 l V k B o q 0 P b 0 T 6 c f k S S g I R C O p e x Z Z g x u i g U w w m l W c R J J Y 0 z G e E h 7 i o Y 4 o N J N Z 7 d k x q F S + s Y g E u q F Y M z U n x 0 p D q S c B r 6 q z D e X i 1 4 u / u f 1 E h i c u y k L 4 w R o S O a D B g k 3 I D L y Y I w + E 5 Q A n y q C i W B q V 4 O M s M A E V H w V F Y K 1 e P J f Y j c b F w 3 r 5 r T W a h Z p l N E B q q I 6 s t A Z a q E r 1 E Y 2 I u g B P a E X 9 K o 9 a s / a m / Y + L y 1 p R c 8 + + g X t 4 x s C p 5 q J < / l a t e x i t >

,

xˆt+1)

L ( at < l a t e x i t s h a 1 _ b a s e 6 4 = " b M t g 3 E O Q o R 3 k j b w 2 v e r d 4 2 r y p I M = " > A A A C A n i c b V B N S 8 N A E N 3 U r 1 q / q t 7 0 E i x C B S l J E d R b w Y s H D x W M L T Q h T L b b d u n m g 9 2 J U E L B i 3 / F i w c V r / 4 K b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w h Z b 1 b R S W l l d W 1 4 r r p Y 3 N r e 2 d 8 u 7 e v Y p T S Z l D Y x H L d g C K C R 4 x B z k K 1 k 4 k g z A Q r B U M r y Z + 6 4 F J x e P o D k c J 8 0 L o R 7 z H K a C W / P K B G w I O K I j s Z l w F H 0 / d A W A G Y x 9 P / H L F q l l T m I v E z k m F 5 G j 6 5 S + 3 G 9 M 0 Z B F S A U p 1 b C t B L w O J n A o 2 L r m p Y g n Q I f R Z R 9 M I Q q a 8 b P r D 2 D z W S t f s x V J X h O Z U / T 2 R Q a j U K A x 0 5 + R i N e 9 N x P + 8 T o q 9 C y / j U Z I i i + h s U S 8 V J s b m J B C z y y W j K E a a A J V c 3 2 r S A U i g q G M r 6 R D s + Z c X i V O v X d b s 2 7 N K o 5 6 n U S S H 5 I h U i U 3 O S Y N c k y Z x C C W P 5 J m 8 k j f j y X g x 3 o 2 P W W v B y G f 2 y R 8 Y n z 8 0 p p d j < / l a t e x i t >

,

aˆt)

xˆ t+1 < l a t e x i t s h a 1 _ b a s e 6 4 = " 0 x + q 3 x F 7 e U b 9 Q x l K b 7 w 4 + 0 b i y L g = " > A A A B 8 3 i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S I I Q k m k o N 4 K X j x W M L b Q h r L Z b t q l m w 9 3 J 8 U S 8 j u 8 e F D x 6 p / x 5 r 9 x 2 + a g r Q 8 G H u / N M D P P T 6 T Q a N v f 1 s r q 2 v r G Z m m r v L 2 z u 7 d f O T h 8 0 H G q G H d Z L G P V 9 q n m U k T c R Y G S t x P F a e h L 3 v J H N 1 O / N e Z K i z i 6 x 0 n C v Z A O I h E I R t F I X n d I M X v K e x m e O 3 m v U r V r 9 g x k m T g F q U K B Z q / y 1 e 3 H L A 1 5 h E x S r T u O n a C X U Y W C S Z 6 X u 6 n m C W U j O u A d Q y M a c u 1 l s 6 N z c m q U P g l i Z S p C M l N / T 2 Q 0 1 H o S + q Y z p D j U i 9 5 U / M / r p B h c e Z m I k h R 5 x O a L g l Q S j M k 0 A d I X i j O U E 0 M o U 8 L c S t i Q K s r Q 5 F Q 2 I T i L L y 8 T 9 6 J 2 X X P u 6 t V G v U i j B M d w A m f g w C U 0 4 B a a 4 A K D R 3 i G V 3 i z x t a L 9 W 5 9 z F t X r G L m C P 7 A + v w B T p 6 R 8 g = = < / l a t e x i t >

aˆ t < l a t e x i t s h a 1 _ b a s e 6 4 = " 8 P q 4 / T 3 q 7 j y S B K y / b F 5 s P U I 8 V f 4 = " > A A A B 7 3 i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 4 K X j x W M L b S h j L Z b t q l m 0 3 Y n Q g l 9 F d 4 8 a D i 1 b / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E p h 0 H W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x 6 M E m m G f d Z I h P d C c F w K R T 3 U a D k n V R z i E P J 2 + H 4 Z u a 3 n 7 g 2 I l H 3 O E l 5 E M N Q i U g w Q C s 9 9 k a A O U z 7 2 K / W 3 L o 7 B 1 0 l X k F q p E C r X / 3 q D R K W x V w h k 2 B M 1 3 N T D H L Q K J j k 0 0 o v M z w F N o Y h 7 1 q q I O Y m y O c H T + m Z V Q Y 0 S r Q t h X S u / p 7 I I T Z m E o e 2 M w Y c m W V v J v 7 n d T O M r o J c q D R D r t h i U Z R J i g m d f U 8 H Q n O G c m I J M C 3 s r Z S N Q A N D m 1 H F h u A t v 7 x K / I v 6 d d 2 7 a 9 S a j S K N M j k h p + S c e O S S N M k t a R G f M B K T Z / J K 3 h z t v D j v z s e i t e Q U M 8 f k D 5 z P H 4 Q k k F 8 = < / l a t e x i t > recurrent st at e < l a t e x i t s h a 1 _ b a s e 6 4 = " w 0 N 0 c O T T 7 j 6 q o j Q o l L X R B + a C d Y g = " > A A A C F X i c b V B P S 8 M w H E 3 n v 1 n / V T 1 6 C Q 7 B i 6 P d R b 0 N v H i c 4 N x g L S N N f 9 3 C 0 r Q k q T D K P o U X v 4 o X D y p e B W 9 + G 7 O t g m 4 + C L y 8 9 3 s k v x d m n C n t u l 9 W Z W V 1 b X 2 j u m l v b e / s 7 j n 7 B 3 c q z S W F N k 1 5 K r s h U c C Z g L Z m m k M 3 k 0 C S k E M n H F 1 N / c 4 9 S M V S c a v H G Q Q J G Q g W M 0 q 0 k f r O m R / C g I m C g t A g J 7 Y E m k t p L r 5 v K 0 0 0 2 D 6 I 6 M f u O z W 3 7 s 6 A l 4 l X k h o q 0 e o 7 n 3 6 U 0 j w x c c q J U j 3 P z X R Q E K k Z 5 T C x / V x B R u i I D K B n q C A J q K C Y r T X B J 0 a J c J x K c 4 T G M / V 3 o i C J U u M k N J M J 0 U O 1 6 E 3 F / 7 x e r u O L o G A i y z U I O n 8 o z j n W K Z 5 2 h C N m a t B 8 b A i h k p m / Y j o k k l D T g b J N C d 7 i y s u k 3 a h f 1 r 2 b R q 3 Z K N u o o i N 0 j E 6 R h 8 5 R E 1 2 j F m o j i h 7 Q E 3 p B r 9 a j 9 W y 9 W e / z 0 Y p V Z g 7 R H 1 g f 3 8 K T n + Y = < / l a t e x i t >

xˆ t+1 < l a t e x i t s h a 1 _ b a s e 6 4 = " 0 x + q 3 x F 7 e U b 9 Q x l K b 7 w 4 + 0 b i y L g = " > A A A B 8 3 i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S I I Q k m k o N 4 K X j x W M L b Q h r L Z b t q l m w 9 3 J 8 U S 8 j u 8 e F D x 6 p / x 5 r 9 x 2 + a g r Q 8 G H u / N M D P P T 6 T Q a N v f 1 s r q 2 v r G Z m m r v L 2 z u 7 d f O T h 8 0 H G q G H d Z L G P V 9 q n m U k T c R Y G S t x P F a e h L 3 v J H N 1 O / N e Z K i z i 6 x 0 n C v Z A O I h E I R t F I X n d I M X v K e x m e O 3 m v U r V r 9 g x k m T g F q U K B Z q / y 1 e 3 H L A 1 5 h E x S r T u O n a C X U Y W C S Z 6 X u 6 n m C W U j O u A d Q y M a c u 1 l s 6 N z c m q U P g l i Z S p C M l N / T 2 Q 0 1 H o S + q Y z p D j U i 9 5 U / M / r p B h c e Z m I k h R 5 x O a L g l Q S j M k 0 A d I X i j O U E 0 M o U 8 L c S t i Q K s r Q 5 F Q 2 I T i L L y 8 T 9 6 J 2 X X P u 6 t V G v U i j B M d w A m f g w C U 0 4 B a a 4 A K D R 3 i G V 3 i z x t a L 9 W 5 9 z F t X r G L m C P 7 A + v w B T p 6 R 8 g = = < / l a t e x i t >

L ( x t+1 < l a t e x i t s h a 1 _ b a s e 6 4 = " S D j S A K s S x y 5 T x n 0 e E G x K C s 9 W e 1 0 = " > A A A C C n i c b V D L S s N A F J 3 U V 6 2 v q E s 3 o U W o K C U p g r o r u H H h o o K x h S a E y X T a D p 0 8 m L m R l p C 9 G 3 / F j Q s V t 3 6 B O / / G S Z u F V g 8 M n D n n X u 6 9 x 4 8 5 k 2 C a X 1 p p a X l l d a 2 8 X t n Y 3 N r e 0 X f 3 7 m S U C E J t E v F I d H 0 s K W c h t Y E B p 9 1 Y U B z 4 n H b 8 8 W X u d + 6 p k C w K b 2 E a U z f A w 5 A N G M G g J E + v O g G G E c E 8 v c 7 q E y + F Y y s 7 c U Y Y 0 k k 2 / x 1 5 e s 1 s m D M Y f 4 l V k B o q 0 P b 0 T 6 c f k S S g I R C O p e x Z Z g x u i g U w w m l W c R J J Y 0 z G e E h 7 i o Y 4 o N J N Z 7 d k x q F S + s Y g E u q F Y M z U n x 0 p D q S c B r 6 q z D e X i 1 4 u / u f 1 E h i c u y k L 4 w R o S O a D B g k 3 I D L y Y I w + E 5 Q A n y q C i W B q V 4 O M s M A E V H w V F Y K 1 e P J f Y j c b F w 3 r 5 r T W a h Z p l N E B q q I 6 s t A Z a q E r 1 E Y 2 I u g B P a E X 9 K o 9 a s / a m / Y + L y 1 p R c 8 + + g X t 4 x s C p 5 q J < / l a t e x i t >

,

xˆt+1)

forward consistency < l a t e x i t s h a 1 _ b a s e 6 4 = " B c p 8 X F 0 m z V z h o 7 7 4 h E e R 8 d 4 B c s 4 = " > A A A C G X i c b V B N S 8 N A E N 3 U r x q / o h 6 9 B I v g q S S 9 q L e C F 4 8 V j C 0 0 o W w 2 k 3 b p Z h N 2 N 0 o I / R 1 e / C t e P K h 4 1 J P / x k 1 b Q V s f D D z e z D D z X p g x K p X j f B m 1 l d W 1 9 Y 3 6 p r m 1 v b O 7 Z + 0 f 3 M o 0 F w Q 8 k r J U 9 E I s g V E O n q K K Q S 8 T g J O Q Q T c c X 1 b 9 7 h 0 I S V N + o 4 o M g g Q P O Y 0 p w U p L A 8 v 1 Q x h S X h L g C s T E j F N x j 0 X k + y Z J u d T 3 g Z P C 9 I F H P y M D q + E 0 n S n s Z e L O S Q P N 0 R l Y H 3 6 U k j z R 6 4 R h K f u u k 6 m g x E J R w m B i + r m E D J M x H k J f U 4 4 T k E E 5 t T a x T 7 Q S 2 f o r X V z Z U / X 3 R o k T K Y s k 1 J M J V i O 5 2 K v E / 3 r 9 X M X n Q U l 5 l l c W Z 4 f i n N k q t a u c 7 I g K I I o V m m A i q P 7 V J i M s M N E Z S F O H 4 C 5 a X i Z e q 3 n R d K 9 b j X Z r n k Y d H a F j d I p c d I b a 6 A p 1 k I c I e k B P 6 A W 9 G o / G s / F m v M 9 G a 8 Z 8 5 x D 9 g f H 5 D S h Q o b o = < / l a t e x i t >

aˆ t < l a t e x i t s h a 1 _ b a s e 6 4 = " 8 P q 4 / T 3 q 7 j y S B K y / b F 5 s P U I 8 V f 4 = " > A A A B 7 3 i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 4 K X j x W M L b S h j L Z b t q l m 0 3 Y n Q g l 9 F d 4 8 a D i 1 b / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E p h 0 H W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x 6 M E m m G f d Z I h P d C c F w K R T 3 U a D k n V R z i E P J 2 + H 4 Z u a 3 n 7 g 2 I l H 3 O E l 5 E M N Q i U g w Q C s 9 9 k a A O U z 7 2 K / W 3 L o 7 B 1 0 l X k F q p E C r X / 3 q D R K W x V w h k 2 B M 1 3 N T D H L Q K J j k 0 0 o v M z w F N o Y h 7 1 q q I O Y m y O c H T + m Z V Q Y 0 S r Q t h X S u / p 7 I I T Z m E o e 2 M w Y c m W V v J v 7 n d T O M r o J c q D R D r t h i U Z R J i g m d f U 8 H Q n O G c m I J M C 3 s r Z S N Q A N D m 1 H F h u A t v 7 x K / I v 6 d d 2 7 a 9 S a j S K N M j k h p + S c e O S S N M k t a R G f M B K T Z / J K 3 h z t v D j v z s e i t e Q U M 8 f k D 5 z P H 4 Q k k F 8 = < / l a t e x i t >

L ( at < l a t e x i t s h a 1 _ b a s e 6 4 = " b M t g 3 E O Q o R 3 k j b w 2 v e r d 4 2 r y p I M = " > A A A C A n i c b V B N S 8 N A E N 3 U r 1 q / q t 7 0 E i x C B S l J E d R b w Y s H D x W M L T Q h T L b b d u n m g 9 2 J U E L B i 3 / F i w c V r / 4 K b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w h Z b 1 b R S W l l d W 1 4 r r p Y 3 N r e 2 d 8 u 7 e v Y p T S Z l D Y x H L d g C K C R 4 x B z k K 1 k 4 k g z A Q r B U M r y Z + 6 4 F J x e P o D k c J 8 0 L o R 7 z H K a C W / P K B G w I O K I j s Z l w F H 0 / d A W A G Y x 9 P / H L F q l l T m I v E z k m F 5 G j 6 5 S + 3 G 9 M 0 Z B F S A U p 1 b C t B L w O J n A o 2 L r m p Y g n Q I f R Z R 9 M I Q q a 8 b P r D 2 D z W S t f s x V J X h O Z U / T 2 R Q a j U K A x 0 5 + R i N e 9 N x P + 8 T o q 9 C y / j U Z I i i + h s U S 8 V J s b m J B C z y y W j K E a a A J V c 3 2 r S A U i g q G M r 6 R D s + Z c X i V O v X d b s 2 7 N K o 5 6 n U S S H 5 I h U i U 3 O S Y N c k y Z x C C W P 5 J m 8 k j f j y X g x 3 o 2 P W W v B y G f 2 y R 8 Y n z 8 0 p p d j < / l a t e x i t >

,

aˆt)

recurrent st at e < l a t e x i t s h a 1 _ b a s e 6 4 = " w 0 N 0 c O T T 7 j 6 q o j Q o l L X R B + a C d Y g = " > A A A C F X i c b V B P S 8 M w H E 3 n v 1 n / V T 1 6 C Q 7 B i 6 P d R b 0 N v H i c 4 N x g L S N N f 9 3 C 0 r Q k q T D K P o U X v 4 o X D y p e B W 9 + G 7 O t g m 4 + C L y 8 9 3 s k v x d m n C n t u l 9 W Z W V 1 b X 2 j u m l v b e / s 7 j n 7 B 3 c q z S W F N k 1 5 K r s h U c C Z g L Z m m k M 3 k 0 C S k E M n H F 1 N / c 4 9 S M V S c a v H G Q Q J G Q g W M 0 q 0 k f r O m R / C g I m C g t A g J 7 Y E m k t p L r 5 v K 0 0 0 2 D 6 I 6 M f u O z W 3 7 s 6 A l 4 l X k h o q 0 e o 7 n 3 6 U 0 j w x c c q J U j 3 P z X R Q E K k Z 5 T C x / V x B R u i I D K B n q C A J q K C Y r T X B J 0 a J c J x K c 4 T G M / V 3 o i C J U u M k N J M J 0 U O 1 6 E 3 F / 7 x e r u O L o G A i y z U I O n 8 o z j n W K Z 5 2 h C N m a t B 8 b A i h k p m / Y j o k k l D T g b J N C d 7 i y s u k 3 a h f 1 r 2 b R q 3 Z K N u o o i N 0 j E 6 R h 8 5 R E 1 2 j F m o j i h 7 Q E 3 p B r 9 a j 9 W y 9 W e / z 0 Y p V Z g 7 R H 1 g f 3 8 K T n + Y = < / l a t e x i t >

forward regularizer < l a t e x i t s h a 1 _ b a s e 6 4 = " i d R 6 i E h T 8 e 9 Q S I E 6 1 V q k 7 + u Z S N Y = " > A A A C G X i c b V B N S 8 N A F N z 4 W e N X 1 K O X Y B E 8 l a Q X 9 V b w 4 r G C s Y U m l M 3 m J V 2 6 2 Y T d j V J D f 4 c X / 4 o X D y o e 9 e S / c d t G 0 N a B h W F m H v v e h D m j U j n O l 7 G 0 v L K 6 t l 7 b M D e 3 t n d 2 r b 3 9 G 5 k V g o B H M p a J b o g l M M r B U 1 Q x 6 O Y C c B o y 6 I T D i 4 n f u Q U h a c a v 1 S i H I M U J p z E l W G m p b 7 l + C A n l J Q G u Q I z N O B N 3 W E S + b w p I C o Y F v Q d h + s C j n 0 j f q j s N Z w p 7 k b g V q a M K 7 b 7 1 4 U c Z K V I 9 T h i W s u c 6 u Q p K L B Q l D M a m X 0 j I M R n i B H q a c p y C D M r p a W P 7 W C u R r b f S j y t 7 q v 6 e K H E q 5 S g N d T L F a i D n v Y n 4 n 9 c r V H w W l J T n h Q J O Z h / F B b N V Z k 9 6 s i M q g C g 2 0 g Q T Q f W u N h l g g Y n u Q J q 6 B H f + 5 E X i N R v n D f e q W W 8 1 q z Z q 6 B A d o R P k o l P U Q p e o j T x E 0 A N 6 Q i / o 1 X g 0 n o 0 3 4 3 0 W X T K q m Q P 0 B 8 b n N x 7 Y o b Q = < / l a t e x i t >

features < l a t e x i t s h a 1 _ b a s e 6 4 = " N n e 3 S l 4 H 0 q p z 7 K L z D l x l O E w O w w E = " > A A A B 7 n i c b V B N T 8 J A E J 3 i F + I X 6 t F L I z H x R F o u 6 o 3 E i 0 d M r J B A Q 7 b L F D Z s t 3 V 3 a k I I f 8 K L B z V e / T 3 e / D c u 0 I O i L 5 n k 5 b 2 Z z M y L M i k M e d 6 X U 1 p b 3 9 j c K m 9 X d n b 3 9 g + q h 0 f 3 J s 0 1 x 4 C n M t W d i B m U Q m F A g i R 2 M o 0 s i S S 2 o / H 1 3 G 8 / o j Y i V X c 0 y T B M 2 F C J W H B G V u r E y C j X a P r V m l f 3 F n D / E r 8 g N S j Q 6 l c / e 4 O U 5 w k q 4 p I Z 0 / W 9 j M I p 0 y S 4 x F m l l x v M G B + z I X Y t V S x B E 0 4 X 9 8 7 c M 6 s M 3 D j V t h S 5 C / X n x J Q l x k y S y H Y m j E Z m 1 Z u L / 3 n d n O L L c C p U l h M q v l w U 5 9 K l 1 J 0 / 7 w 6 E R k 5 y Y g n j W t h b X T 5 i m n G y E V V s C P 7 q y 3 9 J 0 K h f 1 f 3 b R q 3 Z K N I o w w m c w j n 4 c A F N u I E W B M B B w h O 8 w K v z 4 D w 7 b 8 7 7 s r X k F D P H 8 A v O x z f A k 4 / r < / l a t e x i t >

features < l a t e x i t s h a 1 _ b a s e 6 4 = " N n e 3 S l 4 H 0 q p z 7 K L z D l x l O E w O w w E = " > A A A B 7 n i c b V B N T 8 J A E J 3 i F + I X 6 t F L I z H x R F o u 6 o 3 E i 0 d M r J B A Q 7 b L F D Z s t 3 V 3 a k I I f 8 K L B z V e / T 3 e / D c u 0 I O i L 5 n k 5 b 2 Z z M y L M i k M e d 6 X U 1 p b 3 9 j c K m 9 X d n b 3 9 g + q h 0 f 3 J s 0 1 x 4 C n M t W d i B m U Q m F A g i R 2 M o 0 s i S S 2 o / H 1 3 G 8 / o j Y i V X c 0 y T B M 2 F C J W H B G V u r E y C j X a P r V m l f 3 F n D / E r 8 g N S j Q 6 l c / e 4 O U 5 w k q 4 p I Z 0 / W 9 j M I p 0 y S 4 x F m l l x v M G B + z I X Y t V S x B E 0 4 X 9 8 7 c M 6 s M 3 D j V t h S 5 C / X n x J Q l x k y S y H Y m j E Z m 1 Z u L / 3 n d n O L L c C p U l h M q v l w U 5 9 K l 1 J 0 / 7 w 6 E R k 5 y Y g n j W t h b X T 5 i m n G y E V V s C P 7 q y 3 9 J 0 K h f 1 f 3 b R q 3 Z K N I o w w m c w j n 4 c A F N u I E W B M B B w h O 8 w K v z 4 D w 7 b 8 7 7 s r X k F D P H 8 A v O x z f A k 4 / r < / l a t e x i t >

features < l a t e x i t s h a 1 _ b a s e 6 4 = " N n e 3 S l 4 H 0 q p z 7 K L z D l x l O E w O w w E = " > A A A B 7 n i c b V B N T 8 J A E J 3 i F + I X 6 t F L I z H x R F o u 6 o 3 E i 0 d M r J B A Q 7 b L F D Z s t 3 V 3 a k I I f 8 K L B z V e / T 3 e / D c u 0 I O i L 5 n k 5 b 2 Z z M y L M i k M e d 6 X U 1 p b 3 9 j c K m 9 X d n b 3 9 g + q h 0 f 3 J s 0 1 x 4 C n M t W d i B m U Q m F A g i R 2 M o 0 s i S S 2 o / H 1 3 G 8 / o j Y i V X c 0 y T B M 2 F C J W H B G V u r E y C j X a P r V m l f 3 F n D / E r 8 g N S j Q 6 l c / e 4 O U 5 w k q 4 p I Z 0 / W 9 j M I p 0 y S 4 x F m l l x v M G B + z I X Y t V S x B E 0 4 X 9 8 7 c M 6 s M 3 D j V t h S 5 C / X n x J Q l x k y S y H Y m j E Z m 1 Z u L / 3 n d n O L L c C p U l h M q v l w U 5 9 K l 1 J 0 / 7 w 6 E R k 5 y Y g n j W t h b X T 5 i m n G y E V V s C P 7 q y 3 9 J 0 K h f 1 f 3 b R q 3 Z K N I o w w m c w j n 4 c A F N u I E W B M B B w h O 8 w K v z 4 D w 7 b 8 7 7 s r X k F D P H 8 A v O x z f A k 4 / r < / l a t e x i t >

features < l a t e x i t s h a 1 _ b a s e 6 4 = " N n e 3 S l 4 H 0 q p z 7 K L z D l x l O E w O w w E = " > A A A B 7 n i c b V B N T 8 J A E J 3 i F + I X 6 t F L I z H x R F o u 6 o 3 E i 0 d M r J B A Q 7 b L F D Z s t 3 V 3 a k I I f 8 K L B z V e / T 3 e / D c u 0 I O i L 5 n k 5 b 2 Z z M y L M i k M e d 6 X U 1 p b 3 9 j c K m 9 X d n b 3 9 g + q h 0 f 3 J s 0 1 x 4 C n M t W d i B m U Q m F A g i R 2 M o 0 s i S S 2 o / H 1 3 G 8 / o j Y i V X c 0 y T B M 2 F C J W H B G V u r E y C j X a P r V m l f 3 F n D / E r 8 g N S j Q 6 l c / e 4 O U 5 w k q 4 p I Z 0 / W 9 j M I p 0 y S 4 x F m l l x v M G B + z I X Y t V S x B E 0 4 X 9 8 7 c M 6 s M 3 D j V t h S 5 C / X n x J Q l x k y S y H Y m j E Z m 1 Z u L / 3 n d n O L L c C p U l h M q v l w U 5 9 K l 1 J 0 / 7 w 6 E R k 5 y Y g n j W t h b X T 5 i m n G y E V V s C P 7 q y 3 9 J 0 K h f 1 f 3 b R q 3 Z K N I o w w m c w j n 4 c A F N u I E W B M B B w h O 8 w K v z 4 D w 7 b 8 7 7 s r X k F D P H 8 A v O x z f A k 4 / r < / l a t e x i t >

x t < l a t e x i t s h a 1 _ b a s e 6 4 = " r U 0 D d K x k x J D s 4 o K y H 3 i q b p i h i 5 s = " > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r G l t o Q 9 l s N + 3 S z S b s T s Q S + h O 8 e F D x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m C T T j P s s k Y l u h 9 R w K R T 3 U a D k 7 V R z G o e S t 8 L R 9 d R v P X J t R K L u c Z z y I K Y D J S L B K F r p 7 q m H v W r N r b s z k G X i F a Q G B Z q 9 6 l e 3 n 7 A s 5 g q Z p M Z 0 P D f F I K c a B Z N 8 U u l m h q e U j e i A d y x V N O Y m y G e n T s i J V f o k S r Q t h W S m / p 7 I a W z M O A 5 t Z 0 x x a B a 9 q f i f 1 8 k w u g x y o d I M u W L z R V E m C S Z k + j f p C 8 0 Z y r E l l G l h b y V s S D V l a N O p 2 B C 8 x Z e X i X 9 W v 6 p 7 t + e 1 x n m R R h m O 4 B h O w Y M L a M A N N M E H B g N 4 h l d 4 c 6 T z 4 r w 7 H / P W k l P M H M I f O J 8 / 2 o i N q Q = = < / l a t e x i t >

x t +1 < l a t e x i t s h a 1 _ b a s e 6 4 = " I X t y A j 4 N S a M y Z w a x l 3 + f W O M N X Z 8 = " > A A A B 7 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S I I Q k m k o N 4 K X j x W M L b Q h r L Z b t q l m 0 3 Y n Y g l 9 E d 4 8 a D i 1 f / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E p h 0 H W / n Z X V t f W N z d J W e X t n d 2 + / c n D 4 Y J J M M + 6 z R C a 6 H V L D p V D c R 4 G S t 1 P N a R x K 3 g p H N 1 O / 9 c i 1 E Y m 6 x 3 H K g 5 g O l I g E o 2 i l 1 l M v x 3 N v 0 q t U 3 Z o 7 A 1 k m X k G q U K D Z q 3 x 1 + w n L Y q 6 Q S W p M x 3 N T D H K q U T D J J + V u Z n h K 2 Y g O e M d S R W N u g n x 2 7 o S c W q V P o k T b U k h m 6 u + J n M b G j O P Q d s Y U h 2 b R m 4 r / e Z 0 M o 6 s g F y r N k C s 2 X x R l k m B C p r + T v t C c o R x b Q p k W 9 l b C h l R T h j a h s g 3 B W 3 x 5 m f g X t e u a d 1 e v N u p F G i U 4 h h M 4 A w 8 u o Q G 3 0 A Q f G I z g G V 7 h z U m d F + f d + Z i 3 r j j F z B H 8 g f P 5 A 3 e E j y U = < / l a t e x i t >

a t < l a t e x i t s h a 1 _ b a s e 6 4 = " y s q / O Z 9 v g l m 5 x K N I E 5 t o C u H U + 5 M = " > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B i y W R g n o r e P F Y w d h C G 8 p m u 2 m X b j Z h d y K U 0 B / h x Y O K V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q F Q d f 9 d k p r 6 x u b W + X t y s 7 u 3 v 5 B 9 f D o 0 S S Z Z t x n i U x 0 J 6 S G S 6 G 4 j w I l 7 6 S a 0 z i U v B 2 O b 2 d + + 4 l r I x L 1 g J O U B z E d K h E J R t F K b d r P 8 c K b 9 q s 1 t + 7 O Q V a J V 5 A a F G j 1 q 1 + 9 Q c K y m C t k k h r T 9 d w U g 5 x q F E z y a a W X G Z 5 S N q Z D 3 r V U 0 Z i b I J + f O y V n V h m Q K N G 2 F J K 5 + n s i p 7 E x k z i 0 n T H F k V n 2 Z u J / X j f D 6 D r I h U o z 5 I o t F k W Z J J i Q 2 e 9 k I D R n K C e W U K a F v Z W w E d W U o U 2 o Y k P w l l 9 e J f 5 l / a b u 3 T d q z U a R R h l O 4 B T O w Y M r a M I d t M A H B m N 4 h l d 4 c 1 L n x X l 3 P h a t J a e Y O Y Y / c D 5 / A F c / j x A = < / l a t e x i t >

1

(a) < l a t e x i t s h a 1 _ b a s e 6 4 = " o 2 S z y 8 f N w w R e o G Q S 1 S H G J H n c 3 6 Q = " > A A A C F X i c b V B N S 8 N A F N z 4 W e N X 1 K O X x S L U g y U p g n o r e N G D U M H a Q l P K Z v P a L t 1 s w u 5 G K K G / w o t / x Y s H F a + C N / + N m 7 a C t g 4 s D D N v e P s m S D h T 2 n W / r I X F p e W V 1 c K a v b 6 x u b X t 7 O z e q T i V F O o 0 5 r F s B k Q B Z w L q m m k O z U Q C i Q I O j W B w k f u N e 5 C K x e J W D x N o R 6 Q n W J d R o o 3 U c Y 7 9 A H p M Z B S E B j m y S + Q I X 4 k 8 A f g 6 D o H b P o j w x + 4 4 R b f s j o H n i T c l R T R F r e N 8 + m F M 0 8 j E K S d K t T w 3 0 e 2 M S M 0 o h 5 H t p w o S Q g e k B y 1 D B Y l A t b P x W S N 8 a J Q Q d 2 N p n t B 4 r P 5 O Z C R S a h g F Z j I i u q 9 m v V z 8 z 2 u l u n v W z p h I U g 2 C T h Z 1 U 4 5 1 j P O O c M g k U M 2 H h h A q m f k r p n 0 i C T U d K N u U 4 M 2 e P E / q l f J 5 2 b u p F K s n 0 z Y K a B 8 d o B L y 0 C m q o k t U Q 3 V E 0 Q N 6 Q i / o 1 X q 0 n q 0 3 6 3 0 y u m B N M 3 v o D 6 y P b y i r n u o = < / l a t e x i t >

Inverse

Model

x t < l a t e x i t s h a 1 _ b a s e 6 4 = " r U 0 D d K x k x J D s 4 o K y H 3 i q b p i h i 5 s = " > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r G l t o Q 9 l s N + 3 S z S b s T s Q S + h O 8 e F D x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m C T T j P s s k Y l u h 9 R w K R T 3 U a D k 7 V R z G o e S t 8 L R 9 d R v P X J t R K L u c Z z y I K Y D J S L B K F r p 7 q m H v W r N r b s z k G X i F a Q G B Z q 9 6 l e 3 n 7 A s 5 g q Z p M Z 0 P D f F I K c a B Z N 8 U u l m h q e U j e i A d y x V N O Y m y G e n T s i J V f o k S r Q t h W S m / p 7 I a W z M O A 5 t Z 0 x x a B a 9 q f i f 1 8 k w u g x y o d I M u W L z R V E m C S Z k + j f p C 8 0 Z y r E l l G l h b y V s S D V l a N O p 2 B C 8 x Z e X i X 9 W v 6 p 7 t + e 1 x n m R R h m O 4 B h O w Y M L a M A N N M E H B g N 4 h l d 4 c 6 T z 4 r w 7 H / P W k l P M H M I f O J 8 / 2 o i N q Q = = < / l a t e x i t >

xg < l a t e x i t s h a 1 _ b a s e 6 4 = " 0 M A n F I / x q 7 k g W H S M O u V x n i c J Q b A = " > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r G l t o Q 9 l s N + n S z S b s T s R S + h O 8 e F D x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v z K Q w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m D T X j P s s l a l u h 9 R w K R T 3 U a D k 7 U x z m o S S t 8 L h 9 d R v P X J t R K r u c Z T x I K G x E p F g F K 1 0 9 9 S L e 9 W a W 3 d n I M v E K 0 g N C j R 7 1 a 9 u P 2 V 5 w h U y S Y 3 p e G 6 G w Z h q F E z y S a W b G 5 5 R N q Q x 7 1 i q a M J N M J 6 d O i E n V u m T K N W 2 F J K Z + n t i T B N j R k l o O x O K A 7 P o T c X / v E 6 O 0 W U w F i r L k S s 2 X x T l k m B K p n + T v t C c o R x Z Q p k W 9 l b C B l R T h j a d i g 3 B W 3 x 5 m f h n 9 a u 6 d 3 t e a 5 w X a Z T h C I 7 h F D y 4 g A b c Q B N 8 Y B D D M 7 z C m y O d F + f d + Z i 3 l p x i 5 h D + w P n 8 A c b h j Z w = < / l a t e x i t >

a t < l a t e x i t s h a 1 _ b a s e 6 4 = " y s q / O Z 9 v g l m 5 x K N I E 5 t o C u H U + 5 M = " > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B i y W R g n o r e P F Y w d h C G 8 p m u 2 m X b j Z h d y K U 0 B / h x Y O K V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q F Q d f 9 d k p r 6 x u b W + X t y s 7 u 3 v 5 B 9 f D o 0 S S Z Z t x n i U x 0 J 6 S G S 6 G 4 j w I l 7 6 S a 0 z i U v B 2 O b 2 d + + 4 l r I x L 1 g J O U B z E d K h E J R t F K b d r P 8 c K b 9 q s 1 t + 7 O Q V a J V 5 A a F G j 1 q 1 + 9 Q c K y m C t k k h r T 9 d w U g 5 x q F E z y a a W X G Z 5 S N q Z D 3 r V U 0 Z i b I J + f O y V n V h m Q K N G 2 F J K 5 + n s i p 7 E x k z i 0 n T H F k V n 2 Z u J / X j f D 6 D r I h U o z 5 I o t F k W Z J J i Q 2 e 9 k I D R n K C e W U K a F v Z W w E d W U o U 2 o Y k P w l l 9 e J f 5 l / a b u 3 T d q z U a R R h l O 4 B T O w Y M r a M I d t M A H B m N 4 h l d 4 c 1 L n x X l 3 P h a t J a e Y O Y Y / c D 5 / A F c / j x A = < / l a t e x i t >

1

(b) < l a t e x i t s h a 1 _ b a s e 6 4 = " Y c + m c R o D n l O 5 s X R 6 3 / r 0 r j N G x R g = " > A A A C F n i c b V B N S w M x F M z W r 7 p + V T 1 6 C R a h H i y 7 R V B v B Q 9 6 E S p a W + g u J Z u + t q H Z 7 J J k h b L 0 X 3 j x r 3 j x o O J V v P l v T N s V t H U g M M y 8 4 e V N E H O m t O N 8 W b m F x a X l l f y q v b a + s b l V 2 N 6 5 U 1 E i K d R p x C P Z D I g C z g T U N d M c m r E E E g Y c G s H g f O w 3 7 k E q F o l b P Y z B D 0 l P s C 6 j R B u p X S h 7 A f S Y S C k I D X J k l 4 J D f J V w z Y 6 U h h h f 3 N R s D 0 T n x 2 8 X i k 7 Z m Q D P E z c j R Z S h 1 i 5 8 e p 2 I J q G J U 0 6 U a r l O r P 2 U S M 0 o h 5 H t J Q p i Q g e k B y 1 D B Q l B + e n k r h E + M E o H d y N p n t B 4 o v 5 O p C R U a h g G Z j I k u q 9 m v b H 4 n 9 d K d P f U T 5 m I E w 2 C T h d 1 E 4 5 1 h M c l 4 Q 6 T Q D U f G k K o Z O a v m P a J J N R 0 o G x T g j t 7 8 j y p V 8 p n Z f e 6 U q w e Z 2 3 k 0 R 7 a R y X k o h N U R Z e o h u q I o g f 0 h F 7 Q q / V o P V t v 1 v t 0 N G d l m V 3 0 B 9 b H N 3 W 8 n w Y = < / l a t e x i t >

Multi-step

GSP

a t < l a t e x i t s h a 1 _ b a s e 6 4 = " h w X D x s 6 W g y l 7 k V y C F M Q v k g E m K d Q = " > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 4 K X j x W N L b Q h r L Z b t q l m 0 3 Y n Q g l 9 C d 4 8 a D i 1 X / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E p h 0 H W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x 6 N E m m G f d Z I h P d C a n h U i j u o 0 D J O 6 n m N A 4 l b 4 f j m 5 n f f u L a i E Q 9 4 C T l Q U y H S k S C U b T S P e 1 j v 1 p z 6 + 4 c Z J V 4 B a l B g V a / + t U b J C y L u U I m q T F d z 0 0 x y K l G w S S f V n q Z 4 S l l Y z r k X U s V j b k J 8 v m p U 3 J m l Q G J E m 1 L I Z m r v y d y G h s z i U P b G V M c m W V v J v 7 n d T O M r o J c q D R D r t h i U Z R J g g m Z / U 0 G Q n O G c m I J Z V r Y W w k b U U 0 Z 2 n Q q N g R v + e V V 4 l / U r + v e X a P W b B R p l O E E T u E c P L i E J t x C C 3 x g M I R n e I U 3 R z o v z r v z s W g t O c X M M f y B 8 / k D t 5 W N k g = = < / l a t e x i t >

x t < l a t e x i t s h a 1 _ b a s e 6 4 = " r U 0 D d K x k x J D s 4 o K y H 3 i q b p i h i 5 s = " > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r G l t o Q 9 l s N + 3 S z S b s T s Q S + h O 8 e F D x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m C T T j P s s k Y l u h 9 R w K R T 3 U a D k 7 V R z G o e S t 8 L R 9 d R v P X J t R K L u c Z z y I K Y D J S L B K F r p 7 q m H v W r N r b s z k G X i F a Q G B Z q 9 6 l e 3 n 7 A s 5 g q Z p M Z 0 P D f F I K c a B Z N 8 U u l m h q e U j e i A d y x V N O Y m y G e n T s i J V f o k S r Q t h W S m / p 7 I a W z M O A 5 t Z 0 x x a B a 9 q f i f 1 8 k w u g x y o d I M u W L z R V E m C S Z k + j f p C 8 0 Z y r E l l G l h b y V s S D V l a N O p 2 B C 8 x Z e X i X 9 W v 6 p 7 t + e 1 x n m R R h m O 4 B h O w Y M L a M A N N M E H B g N 4 h l d 4 c 6 T z 4 r w 7 H / P W k l P M H M I f O J 8 / 2 o i N q Q = = < / l a t e x i t >

xg < l a t e x i t s h a 1 _ b a s e 6 4 = " 0 M A n F I / x q 7 k g W H S M O u V x n i c J Q b A = " > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r G l t o Q 9 l s N + n S z S b s T s R S + h O 8 e F D x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v z K Q w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m D T X j P s s l a l u h 9 R w K R T 3 U a D k 7 U x z m o S S t 8 L h 9 d R v P X J t R K r u c Z T x I K G x E p F g F K 1 0 9 9 S L e 9 W a W 3 d n I M v E K 0 g N C j R 7 1 a 9 u P 2 V 5 w h U y S Y 3 p e G 6 G w Z h q F E z y S a W b G 5 5 R N q Q x 7 1 i q a M J N M J 6 d O i E n V u m T K N W 2 F J K Z + n t i T B N j R k l o O x O K A 7 P o T c X / v E 6 O 0 W U w F i r L k S s 2 X x T l k m B K p n + T v t C c o R x Z Q p k W 9 l b C B l R T h j a d i g 3 B W 3 x 5 m f h n 9 a u 6 d 3 t e a 5 w X a Z T h C I 7 h F D y 4 g A b c Q B N 8 Y B D D M 7 z C m y O d F + f d + Z i 3 l p x i 5 h D + w P n 8 A c b h j Z w = < / l a t e x i t >

a t < l a t e x i t s h a 1 _ b a s e 6 4 = " y s q / O Z 9 v g l m 5 x K N I E 5 t o C u H U + 5 M = " > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B i y W R g n o r e P F Y w d h C G 8 p m u 2 m X b j Z h d y K U 0 B / h x Y O K V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q F Q d f 9 d k p r 6 x u b W + X t y s 7 u 3 v 5 B 9 f D o 0 S S Z Z t x n i U x 0 J 6 S G S 6 G 4 j w I l 7 6 S a 0 z i U v B 2 O b 2 d + + 4 l r I x L 1 g J O U B z E d K h E J R t F K b d r P 8 c K b 9 q s 1 t + 7 O Q V a J V 5 A a F G j 1 q 1 + 9 Q c K y m C t k k h r T 9 d w U g 5 x q F E z y a a W X G Z 5 S N q Z D 3 r V U 0 Z i b I J + f O y V n V h m Q K N G 2 F J K 5 + n s i p 7 E x k z i 0 n T H F k V n 2 Z u J / X j f D 6 D r I h U o z 5 I o t F k W Z J J i Q 2 e 9 k I D R n K C e W U K a F v Z W w E d W U o U 2 o Y k P w l l 9 e J f 5 l / a b u 3 T d q z U a R R h l O 4 B T O w Y M r a M I d t M A H B m N 4 h l d 4 c 1 L n x X l 3 P h a t J a e Y O Y Y / c D 5 / A F c / j x A = < / l a t e x i t >

1

(c) < l a t e x i t s h a 1 _ b a s e 6 4 = " Q s s F B y k 0 c u c C C Z p h g 4 F x 0 + O m J U k = " > A A A C H 3 i c b V D L S g M x F M 3 4 r O N r 1 K W b Y B H q w j J T h O q u I K j L i t Y W 2 l I y m d s 2 N J M Z k o x S h 3 6 K G 3 / F j Q s V c d e / M X 0 I 2 n o g c D j n X G 7 u 8 W P O l H b d o b W w u L S 8 s p p Z s 9 c 3 N r e 2 n Z 3 d O x U l k k K F R j y S N Z 8 o 4 E x A R T P N o R Z L I K H P o e r 3 z k d + 9 R 6 k Y p G 4 1 f 0 Y m i H p C N Z m l G g j t Z x i w 4 c O E y k F o U E O 7 B w 9 w h e R f C A y O J b Q S T i R 7 B E C f H l T t h s g g p 9 g y 8 m 6 e X c M P E + 8 K c m i K c o t 5 6 s R R D Q J z T j l R K m 6 5 8 a 6 m R K p G e U w s B u J g p j Q H u l A 3 V B B Q l D N d H z g A B 8 a J c D t S J o n N B 6 r v y d S E i r V D 3 2 T D I n u q l l v J P 7 n 1 R P d P m 2 m T M S J B k E n i 9 o J x z r C o 7 Z w w C R Q z f u G E C q Z + S u m X S I J N R 0 o 2 5 T g z Z 4 8 T y q F / F n e u y 5 k S y f T N j J o H x 2 g H P J Q E Z X Q F S q j C q L o C b 2 g N / R u P V u v 1 o f 1 O Y k u W N O Z P f Q H 1 v A b A i C j D Q = = < / l a t e x i t >

Forward-regularized

GSP

x t < l a t e x i t s h a 1 _ b a s e 6 4 = " r U 0 D d K x k x J D s 4 o K y H 3 i q b p i h i 5 s = " > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r G l t o Q 9 l s N + 3 S z S b s T s Q S + h O 8 e F D x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m C T T j P s s k Y l u h 9 R w K R T 3 U a D k 7 V R z G o e S t 8 L R 9 d R v P X J t R K L u c Z z y I K Y D J S L B K F r p 7 q m H v W r N r b s z k G X i F a Q G B Z q 9 6 l e 3 n 7 A s 5 g q Z p M Z 0 P D f F I K c a B Z N 8 U u l m h q e U j e i A d y x V N O Y m y G e n T s i J V f o k S r Q t h W S m / p 7 I a W z M O A 5 t Z 0 x x a B a 9 q f i f 1 8 k w u g x y o d I M u W L z R V E m C S Z k + j f p C 8 0 Z y r E l l G l h b y V s S D V l a N O p 2 B C 8 x Z e X i X 9 W v 6 p 7 t + e 1 x n m R R h m O 4 B h O w Y M L a M A N N M E H B g N 4 h l d 4 c 6 T z 4 r w 7 H / P W k l P M H M I f O J 8 / 2 o i N q Q = = < / l a t e x i t >

xg < l a t e x i t s h a 1 _ b a s e 6 4 = " 0 M A n F I / x q 7 k g W H S M O u V x n i c J Q b A = " > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r G l t o Q 9 l s N + n S z S b s T s R S + h O 8 e F D x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v z K Q w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m D T X j P s s l a l u h 9 R w K R T 3 U a D k 7 U x z m o S S t 8 L h 9 d R v P X J t R K r u c Z T x I K G x E p F g F K 1 0 9 9 S L e 9 W a W 3 d n I M v E K 0 g N C j R 7 1 a 9 u P 2 V 5 w h U y S Y 3 p e G 6 G w Z h q F E z y S a W b G 5 5 R N q Q x 7 1 i q a M J N M J 6 d O i E n V u m T K N W 2 F J K Z + n t i T B N j R k l o O x O K A 7 P o T c X / v E 6 O 0 W U w F i r L k S s 2 X x T l k m B K p n + T v t C c o R x Z Q p k W 9 l b C B l R T h j a d i g 3 B W 3 x 5 m f h n 9 a u 6 d 3 t e a 5 w X a Z T h C I 7 h F D y 4 g A b c Q B N 8 Y B D D M 7 z C m y O d F + f d + Z i 3 l p x i 5 h D + w P n 8 A c b h j Z w = < / l a t e x i t >

a t < l a t e x i t s h a 1 _ b a s e 6 4 = " y s q / O Z 9 v g l m 5 x K N I E 5 t o C u H U + 5 M = " > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B i y W R g n o r e P F Y w d h C G 8 p m u 2 m X b j Z h d y K U 0 B / h x Y O K V / + P N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q F Q d f 9 d k p r 6 x u b W + X t y s 7 u 3 v 5 B 9 f D o 0 S S Z Z t x n i U x 0 J 6 S G S 6 G 4 j w I l 7 6 S a 0 z i U v B 2 O b 2 d + + 4 l r I x L 1 g J O U B z E d K h E J R t F K b d r P 8 c K b 9 q s 1 t + 7 O Q V a J V 5 A a F G j 1 q 1 + 9 Q c K y m C t k k h r T 9 d w U g 5 x q F E z y a a W X G Z 5 S N q Z D 3 r V U 0 Z i b I J + f O y V n V h m Q K N G 2 F J K 5 + n s i p 7 E x k z i 0 n T H F k V n 2 Z u J / X j f D 6 D r I h U o z 5 I o t F k W Z J J i Q 2 e 9 k I D R n K C e W U K a F v Z W w E d W U o U 2 o Y k P w l l 9 e J f 5 l / a b u 3 T d q z U a R R h l O 4 B T O w Y M r a M I d t M A H B m N 4 h l d 4 c 1 L n x X l 3 P h a t J a e Y O Y Y / c D 5 / A F c / j x A = < / l a t e x i t >

1

(d) < l a t e x i t s h a 1 _ b a s e 6 4 = " A v M 0 k 9 A 7 Q n q k f A C e B S / 9 I O K X x 8 o = " > A A A C J 3 i c b V B N S w M x E M 3 6 W d e v q k c v w S K 0 B 8 u u C O p J Q V C P F a 0 W u q V k s 9 M 2 N J s s S V Y p S 3 + O F / + K F x E V P f p P T D 8 E t Q 4 M P N 6 b x 8 y 8 M O F M G 8 / 7 c K a m Z 2 b n 5 n M L 7 u L S 8 s p q f m 3 9 W s t U U a h S y a W q h U Q D Z w K q h h k O t U Q B i U M O N 2 H 3 Z K D f 3 I L S T I o r 0 0 u g E Z O 2 Y C 1 G i b F U M 3 8 U h N B m I q M g D K i + W 4 x K + F S q O 6 K i H S q F t h d Y B Z 9 d V o L A L d q t u u Q G I K J v Q z N f 8 M r e s P A k 8 M e g g M Z V a e a f g 0 j S N L Z 2 y o n W d d 9 L T C M j y j D K o e 8 G q Y a E 0 C 5 p Q 9 1 C Q W L Q j W z 4 a B 9 v W y b C L a l s 2 6 u G 7 E 9 H R m K t e 3 F o J 2 N i O v q v N i D / 0 + q p a R 0 0 M i a S 1 L 5 L R 4 t a K c d G 4 k F q O G I K q O E 9 C w h V z N 6 K a Y c o Q m 0 G 2 r U h + H 9 f n g T V 3 f J h 2 b / Y L R z v j d P I o U 2 0 h Y r I R / v o G J 2 j C q o i i u 7 R I 3 p B r 8 6 D 8 + S 8 O e + j 0 S l n 7 N l A v 8 r 5 / A K n c a X m < / l a t e x i t >

Forward-consistent (ours)

GSP

Figure 1: The goal-conditioned skill policy (GSP) takes as input the current and goal observations and outputs an action sequence that would lead to that goal. We compare the performance of the following GSP models: (a) Simple inverse model; (b) Mutli-step GSP with previous action history; (c) Mutli-step GSP with previous action history and a forward model as regularizer, but no forward consistency; (d) Mutli-step GSP with forward consistency loss proposed in this work.

inferred from the observations. The agent then infers how to perform the task (i.e., plan for imitation) using this state. Unfortunately, computer vision systems are often unable to estimate the state variables accurately and it has proven nontrivial for downstream planning systems to be robust to such errors. In this paper, we follow (Agrawal et al. , 2016; Levine et al. , 2016; Pinto & Gupta, 2016) in pursuing an alternative paradigm, where an agent explores the environment without any expert supervision and distills this exploration data into goaldirected skills. These skills can then be used to imitate the visual demonstration provided by the expert ( Nair et al. , 2017). Here, by skill we mean a function that predicts the sequence of actions to take the agent from the current observation to the goal. We call this function a goal-conditioned skill policy ( GSP). The GSP is learned in a self-supervised way by relabeling the states visited during the agent’s exploration of the environment as goals and the actions executed by the agent as the prediction targets, similar to (Agrawal et al. , 2016; Andrychowicz et al. , 2017). During inference, given goal observations from a demonstration, the GSP can infer how to reach these goals in turn from the current observation, and thereby imitate the task step-by-step. One critical challenge in learning the GSP is that, in general, there are multiple possible ways of going from one state to another: that is, the distribution of trajectories between states is multimodal. We address this issue with our novel forward consistency loss based on the intuition that, for most tasks, reaching the goal is more important than how it is reached. To operationalize this, we ﬁrst learn a forward model that predicts the next observation given an action and a current observation. We use the difference in the output of the forward model for the GSPselected action and the ground truth next state to train the GSP. This loss has the effect of making the GSP-predicted action consistent with the groundtruth action instead of exactly matching the actions themselves, thus ensuring that actions that are different from the groundtruth—but lead to the same next state— are not inadvertently penalized. To account for varying number of steps required to reach different goals, we propose to jointly optimize the GSP with a goal recognizer that determines if the current goal has been satisﬁed. See Figure 1 for a schematic illustration of the GSP architecture. We call our method zero-shot because the agent never has access to expert actions, neither during training of the GSP nor for task demonstration at inference. In contrast, most recent work on oneshot imitation learning requires full knowledge of actions and a wealth of expert demonstrations during training ( Duan et al. , 2017; Finn et al. , 2017). In summary, we propose a method that (1) does not require any extrinsic reward or expert supervision during learning, (2) only needs demonstrations during inference, and ( 3) restricts demonstrations to visual observations alone rather than full stateactions. Instead of learning by imitation, our agent learns to imitate. 2

Published as a conference paper at ICLR 2018

We evaluate our zero-shot imitator on real-world robots for rope manipulation tasks using a Baxter and ofﬁce navigation using a TurtleBot. We show that the proposed forward consistency loss improves the performance on the complex task of knot tying from 36% to 60% accuracy. In navigation experiments, we steer a simple wheeled robot around partiallyobservable ofﬁce environments and show that the learned GSP generalizes to unseen environments. Furthermore, using navigation experiments in VizDoom environment, we show that (GSP) learned using curiosity-driven exploration ( Oudeyer et al. , 2007; Pathak et al. , 2017; Schmidhuber, 1991) can more accurately follow demonstrations as compared to using random exploration data for learning the GSP. Overall our experiments show that the forward-consistent GSP can be used to imitate a variety of tasks without making environment or taskspeciﬁc assumptions.

2 LEARNING TO IMITATE WITHOUT EXPERT SUPERVISION

Let S : {x1, a1, x2, a2, ..., xT } be the sequence of observations and actions generated by the agent as it explores its environment using the policy a = πE(s). This exploration data is used to learn the goalconditioned skill policy (GSP) π takes as input a pair of observations ( xi, xg) and outputs sequence of actions (aτ : a1, a2...aK ) required to reach the goal observation (xg) from the current observation ( xi).

aτ = π(xi, xg; θπ)

(1)

where states xi, xg are sampled from the S. The number of actions, K, is also inferred by the model. We represent π by a deep network with parameters θπ in order to capture complex mappings from visual observations ( x) to actions. π can be thought of as a variable-step generalization of the inverse dynamics model ( Jordan & Rumelhart, 1992), or as the policy corresponding to a universal value function ( Foster & Dayan, 2002; Schaul et al. , 2015), with the difference that xg need not be the end goal of a task but can also be an intermediate subgoal.

Let

the

task

to

be

imitated

be

provided

as

a

sequence

of

images

D

:

{xd1 ,

xd2 ,

. . . ,

xd N

}

captured

when

the expert demonstrates the task. This sequence of images D could either be temporally dense or

sparse. Our agent uses the learned GSP π to imitate the sequence of visual observations D starting

from

its

initial

state

x0

by

following

actions

predicted

by

π

(x

0

,

xd 1

;

θ

π

)

.

Let

the

observation

after

executing the predicted action be x0. Since multiple actions might be required to reach close to

xd1, the agent queries a separate goal recognizer network to ascertain if the current observation is

close

to

the

goal

or

not.

If

the

answer

is

negative,

the

agent

executes

the

action

a

=

π

(

x

0

,

xd 1

;

θ

π

).

This process is repeated iteratively until the goal recognizer outputs that agent is near the goal, or a

maximum number of steps are reached. Let the observation of the agent at this point be xˆ1. After

reaching close to the ﬁrst observation (xd1) in the demonstration, the agent sets its goal as (xd2) and

repeats the process. The agent stops when all observations in the demonstrations are processed.

Note that in the method of imitation described above, the expert is never required to convey to the agent what actions it performed. In the following subsections we describe how we learn the GSP, forward consistency loss, goal recognizer network and various baseline methods.

2. 1 LEARNING THE GOAL-CONDITIONED SKILL POLICY (GSP)

We ﬁrst describe the one-step version of GSP and then extend it to variable length multistep skills. One-step trajectories take the form of (xt, at, xt+ 1) and GSP, aˆt = π(xt, xt+1; θπ), is trained by minimizing the standard crossentropy loss L(at, aˆt),

L(at, aˆt) = p(at|xt, xt+1) log(aˆt)

(2)

with respect to parameters θπ, where p and aˆt are the groundtruth and predicted action distributions. While we do not have access to true p, we empirically approximate it using samples from the distribution, at, that are executed by the agent during exploration. For minimizing the crossentropy loss, it is common to assume p as a delta function at at. However, this assumption is notably violated if p is inherently multimodal and high-dimensional. If we optimize say a deep neural network assuming p to be a delta function, the same inputs will be presented with different targets ( due to multi-modality) leading to high-variance in gradients which in turn would make learning challenging.

3

Published as a conference paper at ICLR 2018

In our setup, such multi-modality can occur because multiple actions can lead the agent to the same future observation from the initial observation. For instance, in navigation, if the agent is stuck against a corner, turning or moving forward all collapse to the same effect. The issue of multimodality becomes more critical as the length of trajectories grow, because more and more paths may take the agent from the initial observation to the goal observation given more time. Furthermore, it would require many samples to even obtain a good empirical estimate of a highdimensional multimodal action distribution p.

2. 2 FORWARD CONSISTENCY LOSS

One way to account for multi-modality is by employing the likes of variational autoencoders ( Kingma & Welling, 2013; Rezende et al. , 2014). However, in many practical situations it is not feasible to obtain ample data for each mode. In this work, we propose an alternative based on the insight that in many scenarios, we only care about whether the agent reached the ﬁnal state or not and the exact trajectory is of lesser interest. Instead of penalizing the actions predicted by the GSP to match the ground truth, we propose to learn the parameters of GSP by minimizing the distance between observation xˆt+1 resulting by executing the predicted action aˆt = π(xt, xt+1; θπ) and the observation xt+1, which is the result of executing the ground truth action at being used to train the GSP. In this formulation, even if the predicted and groundtruth action are different, the predicted action will not be penalized if it leads to the same next state as the groundtruth action. While this formulation will not explicitly maintain all modes of the action distribution, it will reduce the variance in gradients and thus help learning. We call this penalty the forward consistency loss.

Note that it is not immediately obvious as to how to operationalize forward consistency loss for two reasons: (a) we need the access to a good forward dynamics model that can reliably predict the effect of an action ( i.e., the next observation state) given the current observation state, and (b) such a dynamics model should be differentiable in order to train the GSP using the state prediction error. Both of these issues could be resolved if an analytic formulation of forward dynamics is known.

In many scenarios of interest, especially if states are represented as images, an analytic forward

model is not available. In this work, we learn the forward dynamics f model from the data, and

is deﬁned as x˜t+1 = f (xt, at; θf ). Let xˆt+1 = f (xt, aˆt; θf ) be the state prediction for the action

predicted by π. Because the forward model is not analytic and learned from data, in general, there

is no guarantee that x˜t+1 = xˆt+1, even though executing these two actions, at, aˆt, in the real-world

will have the same effect. In order to make the outcome of action predicted by the GSP and the

ground-truth action to be consistent with each other, we include an additional term,

xt+1 − xˆt+1

2 2

in our loss function and infer the parameters θf by minimizing

xt+1 − x˜t+1

2 2

+

λ xt+1 −

xˆt+1

2, 2

where

λ

is

a

scalar

hyper-parameter.

The ﬁrst term ensures that the learned forward model

explains ground truth transitions (xt, at, xt+1) collected by the agent and the second term ensures

consistency. The joint objective for training GSP with forward model consistency is:

min

xt+1 − x˜t+1

2 2

+

λ

xt+1 − xˆt+1

2 2

+

L(at, aˆt)

(3)

θπ ,θf

s. t. x˜t+1 = f (xt, at; θf )

xˆt+1 = f (xt, aˆt; θf )

aˆt = π(xt, xt+1; θπ) Note that learning θπ, θf jointly from scratch is precarious, because the forward model f might not be good in the beginning, and hence could make the gradient updates noisier for π. To address this issue, we ﬁrst pre-train the forward model with only the ﬁrst term and GSP separately by blocking the gradient ﬂow and then ﬁnetune jointly.

Generalization to feature space dynamics Past work has shown that learning forward dynamics in the feature space as opposed to raw observation space is more robust and leads to better generalization ( Agrawal et al. , 2016; Pathak et al. , 2017). Following these works, we extend the GSP to make predictions in feature representation φ( xt), φ(xt+1) of the observations xt, xt+1 respectively learned through the selfsupervised task of action prediction. The forward consistency loss is then computed by making predictions in this feature space φ instead of raw observations. The optimization objective for feature space generalization with mutlistep objective is shown in Equation (4).

Generalization to multi-step GSP We extend our onestep optimization to variable length sequence of actions in a straightforward manner by having a multistep GSP πm model with a step 4

Published as a conference paper at ICLR 2018

wise forward consistency loss. The GSP πm maintains an internal recurrent memory of the system and outputs actions conditioned on current observation xt, starting from xi to reach goal observation xT . The forward consistency loss is computed at each time step, and jointly optimized with the action prediction loss over the whole trajectory. The ﬁnal multi-step objective with feature space dynamics is as follows:

t=T

min

φ(xt+1) − φ˜(xt+1)

2 2

+

λ

φ(xt+1) − φˆ(xt+1)

2 2

+

L(at, aˆt)

(4)

θπ ,θf ,θφ t=i

s. t. φ˜(xt+1) = f (φ(xt), at; θf )

φˆ(xt+1) = f (φ(xt), aˆt; θf )

aˆt = π(φ(xt), φ(xT ); θπ) where φ(. ) is represented by a CNN with parameters θφ. The number of steps taken by the multistep GSP πm to reach the goal at inference is variable depending on the decision of goal recognizer; described in next subsection. Note that, in this objective, if φ is identity then the dynamics simply reduces to modeling in raw observation space. We analyze feature space prediction in VizDoom 3D navigation and stick to observation space in the rope manipulation and the ofﬁce navigation tasks.

The multi-step forward-consistent GSP πm is implemented using a recurrent network which at every step takes as input the feature representation of the current ( φ(xt)) state, goal (φ(xT )) states, action at the previous time step ( at−1) and the internal hidden representation ht−1 of the recurrent units and predicts aˆt. Note that inputting the previous action to GSP πm at each time step could be redundant given that hidden representation is already maintaining a history of the trajectory. Nonetheless, it is helpful to explicitly model this history. This formulation amounts to building an auto-regressive model of the joint action that estimates probability P (at| x1, a1, ...at−1, xt, xg) at every time step. It is possible to further extend our forward-consistent GSP πm to build multistep forward model, but we leave that direction of future work.

2. 3 GOAL RECOGNIZER We train a goal recognizer network to ﬁgure out if the current goal is reached and therefore allow the agent to take variable numbers of steps between goals. Goal recognition is especially critical when the agent has to transit through a sequence of intermediate goals, as is the case for visual imitation, as otherwise compounding error could quickly lead to divergence from the demonstration. This recognition is simple given knowledge of the true physical state, but difﬁcult when working with visual observations. Aside from the usual challenges of visual recognition, the dependence of observations on the agent’s own dynamics further complicates goal recognition, as the same goal can appear different while moving forward or turning during navigation. We pose goal recognition as a binary classiﬁcation problem that given an observation xi and the goal xg infers if xi is close to xg or not. Lacking expert supervision of goals, we draw goal observations at random from the agent’s experience during exploration, since they are known to be feasible. For each such pseudo-goal, we consider observations that were only a few actions away to be positives ( i.e., close to the goal) and the remaining observations that were more than a ﬁxed number of actions ( i.e., a margin) away as negatives. We trained the goal classiﬁer using the standard crossentropy loss. Like the skill policy, our goal recognizer is conditioned on the goal for generalization across goals. We found that training an independent goal recognition network consistently outperformed the alternative approach that augments the action space with a “stop” action. Making use of temporal proximity as supervision has also been explored for feature learning in the concurrent work of Sermanet et al. (2018).

2. 4 ABLATIONS AND BASELINES Our proposed formulation of GSP composed of following components: (a) recurrent variable-length skill policy network, (b) explicitly encoding previous action in the recurrence, (c) goal recognizer, (d) forward consistency loss function, and (w) learning forward dynamics in the feature space instead of raw observation space. We systematically ablate these components of forwardconsistent GSP, to quantitatively review the importance of each component and then perform comparisons to the prior approaches that could be deployed for the task of visual imitation.

5

Robot Success Human Demo Robot Failure Robot Success Human Demo

Published as a conference paper at ICLR 2018 Camera to capture  RGB images

Step-1

Step-2

Step-3

Step-4

One end of the rope is ﬁxed. Baxter Robot

Step-1

(a) < l a t e x i t s h a 1 _ b a s e 6 4 = " 7 K V 8 Y n O 3 V Z y R o R U A P 3 A 5 q g / C P V E = " > A A A B + n i c b V D L T g I x F L 2 D L 8 Q X 4 t J N I z H B D Z l h o + 5 I 3 L j E x w g J T E i n d K C h 0 0 7 a j p F M + B U 3 L t S 4 9 U v c + T c W m I W C J 7 n J y T n 3 t v e e M O F M G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + b D y o G W q C P W J 5 F J 1 Q q w p Z 4 L 6 h h l O O 4 m i O A 4 5 b Y f j q 5 n f f q R K M y n u z S S h Q Y y H g k W M Y G O l f r l S w 2 f o V o b S M K L R H T V p 0 i 9 X 3 b o 7 B 1 o l X k 6 q k K P V L 3 / 1 B p K k M R W G c K x 1 1 3 M T E 2 R Y 2 T c 5 n Z Z 6 q a Y J J m M 8 p F 1 L B Y 6 p D r L 5 7 l N 0 a p U B i q S y J Q y a q 7 8 n M h x r P Y l D 2 x l j M 9 L L 3 k z 8 z + u m J r o I M i a S 1 F B B F h 9 F K U d G o l k Q a M A U J Y Z P L M F E M b s r I i O s M D E 2 r p I N w V s + e Z X 4 j f p l 3 b t p V J u N P I 0 i H M M J 1 M C D c 2 j C N b T A B w J P 8 A y v 8 O Z M n R f n 3 f l Y t B a c f O Y I / s D 5 / A E 1 b Z N p < / l a t e x i t >

Robotics

Setup

Step-2

Step-3

Step-4

(b) < l a t e x i t s h a 1 _ b a s e 6 4 = " j a X w a x C 8 1 2 r u 5 X b t T q E A D R p d x H g = " > A A A C D 3 i c b V D N T g I x G O z i H + L f q k c v j c S I F 7 L L R b 2 R e P F i g o k r J E B I t x R o 6 L Z N + 6 0 J I T y C F 1 / F i w c 1 X r 1 6 8 2 0 s C w c F J 2 k y n f m + t D O x F t x C E H x 7 u Z X V t f W N / G Z h a 3 t n d 8 / f P 7 i 3 K j W U R V Q J Z R o x s U x w y S L g I F h D G 0 a S W L B 6 P L y a + v U H Z i x X 8 g 5 G m r U T 0 p e 8 x y k B J 3 X 8 0 1 J 8 h m + I 5 D o V T p J 9 b J R m m E t Q G E b T O 8 F D q a D j F 4 N y k A E v k 3 B O i m i O W s f / a n U V T R M m g Q p i b T M M N L T H x A C n g k 0 K r d Q y T e i Q 9 F n T U U k S Z t v j L N A E n z i l i 3 v K u C M B Z + r v j T F J r B 0 l s Z t M C A z s o j c V / / O a K f Q u 2 m M u d Q p M 0 t l D v V T g L K 3 L 3 e W G U R A j R w g 1 3 P 0 V 0 w E x h I L r s O B K C B c j L 5 O o U r 4 s h 7 e V Y r U y b y O P j t A x K q E Q n a M q u k Y 1 F C G K H t E z e k V v 3 p P 3 4 r 1 7 H 7 P R n D f f O U R / 4 H 3 + A O D i n A c = < / l a t e x i t >

Manipulating

rope

into

tying

a

knot

Step-5

Step-6

Step-7

Step-8

(c) < l a t e x i t s h a 1 _ b a s e 6 4 = " x A R H E Q w j l L u / o P q r 4 X K y e 1 c 1 k 6 c = " > A A A C D H i c b V D L T g I x F O 3 4 R H y h L t 0 0 o h E 3 Z I a N u i N x 4 8 Y E o w g J E O y U C z R 0 2 q b t m J A J P + D G X 3 H j Q o 1 b P 8 C d f 2 O B W S h 4 k i Y n 5 9 y b 2 3 N C x Z m x v v / t L S w u L a + s Z t a y 6 x u b W 9 u 5 n d 0 7 I 2 N N o U o l l 7 o e E g O c C a h a Z j n U l Q Y S h R x q 4 e B i 7 N c e Q B s m x a 0 d K m h F p C d Y l 1 F i n d T O H R b o C b 4 i g q m Y O 0 n 0 s J Y K M B N W 4 v u b Y 2 z 6 R E E 7 l / e L / g R 4 n g Q p y a M U l X b u q 9 m R N I 5 A W M q J M Y 3 A V 7 a V E G 0 Z 5 T D K N m M D i t A B 6 U H D U U E i M K 1 k k m a E j 5 z S w V 2 p 3 R M W T 9 T f G w m J j B l G o Z u M i O 2 b W W 8 s / u c 1 Y t s 9 a y V M q N i C o N N D 3 Z h j F 3 V c D e 4 w D d T y o S O E a u b + i m m f a E K t K z D r S g h m I 8 + T a q l 4 X g y u S / l y K W 0 j g / b R A S q g A J 2 i M r p E F V R F F D 2 i Z / S K 3 r w n 7 8 V 7 9 z 6 m o w t e u r O H / s D 7 / A E H n J p t < / l a t e x i t >

Manipulating

rope

into

‘S’

shape

Figure 2: Qualitative visualization of results for rope manipulation task using Baxter robot. (a) Our robotics system setup. (b) The sequence of human demonstration images provided by the human during inference for the task of knottying (top row), and the sequences of observation states reached by the robot while imitating the given demonstration ( bottom rows). (c) The sequence of human demonstration images and the ones reached by the robot for the task of manipulating rope into ‘S’ shape. Our agent is able to successfully imitate the demonstration.

The following methods will be evaluated and compared to in the subsequent experiments section: (1) Classical methods: In visual navigation, we attempted to compare against the stateof-the-art open source classical methods, namely, ORBSLAM2 (Davison & Murray, 1998; Mur-Artal & Tardo´s, 2017) and OpenSFM (Mapillary, 2016). (2) Inverse Model: Nair et al. (2017) leverage vanilla inverse dynamics to follow demonstration in rope manipulation setup. We compare to their method in both visual navigation and manipulation. (3) GSP-NoPrevAction-NoFwdConst is the ablation of our recurrent GSP without previous action history and without forward consistency loss. (4) GSP-NoFwdConst refers to our recurrent GSP with previous action history, but without forward consistency objective. (5) GSP-FwdRegularizer refers to the model where forward prediction is only used to regularize the features of GSP but has no role to play in the loss function of predicted actions. The purpose of this variant is to particularly ablate the beneﬁt of consistency loss function with respect to just having forward model as feature regularizer. (6) GSP refers to our complete method with all the components. We now discuss the experiments and evaluate these baselines. 3 EXPERIMENTS We evaluate our model by testing its performance on: rope manipulation using Baxter robot, navigation of a wheeled robot in cluttered ofﬁce environments, and simulated 3D navigation. The key requirements of a good skill policy are that it should generalize to unseen environments and new goals while staying robust to irrelevant distractors in the observations. For rope manipulation, we evaluate generalization by testing the ability of the robot to manipulate the rope into conﬁgurations such as knots that were not seen during random exploration. For navigation, both real-world and simulation, we check generalization by testing on a novel building/ ﬂoor. 3. 1 ROPE MANIPULATION Manipulation of non-rigid and deformable objects, e.g., rope, is a challenging problem in robotics. Even humans learn complex rope manipulation such as tying knots, either by observing an expert

6

Published as a conference paper at ICLR 2018

Published as a conference paper at ICLR 2018

Method Inverse Model [Nair et. al. 2017] Forward-regularized GSP Forward-consistent GSP [ Ours]

Success % 36% ± 9. 6% 44% ± 9. 9% 60% ± 9. 8%

Table 1: GSP trained using forward consistency loss, signiﬁcantly outperforms the baseline GSP model ( i.e. inverse model only) at knot tying.

(a) < l a t e x i t s h a 1 _ b a s e 6 4 = " + M w V k N 1 O A H 6 7 H m y g Y Y u F U p I d 5 0 Q = " > A A A C F H i c b V C 7 S g N B F J 3 1 G e M r a m k z G E Q F D b s 2 a h e w s R F W k z V C E v T u 5 K 4 Z n J 1 d Z m a F E P I T N v 6 K j Y W K r Y W d f + M k 2 c L X g Y H D O e c y 9 5 4 w F V w b 1 / 1 0 J i a n p m d m C 3 P F + Y X F p e X S y u q F T j L F M G C J S N R l C B o F l x g Y b g R e p g o h D g U 2 w t v j o d + 4 Q 6 V 5 I u u m l 2 I 7 h h v J I 8 7 A W O m q t L s N O 7 T u 1 / b O / V O K S i W K R v Z d 1 7 a o 7 k K K N A b J 0 0 z k 8 b J b c U e g f 4 m X k z L J 4 V + V P l q d h G U x S s M E a N 3 0 3 N S 0 + 6 A M Z w I H x V a m M Q V 2 C z f Y t F R C j L r d H 1 0 1 o J t W 6 Y y 2 i R J p 6 E j 9 P t G H W O t e H N p k D K a r f 3 t D 8 T + v m Z n o s N 3 n M s 0 M S j b + K M o E N Q k d V k Q 7 X C E z o m c J M M X t r p R 1 Q Q E z t s i i L c H 7 f f J f E u x X j i r e 2 X 6 5 6 u Z t F M g 6 2 S D b x C M H p E p O i E 8 C w s g 9 e S T P 5 M V 5 c J 6 c V + d t H J 1 w 8 p k 1 8 g P O + x c L B 5 0 V < / l a t e x i t >

TPS-RPM

error

for

3‘S.’1shapRe OmPaEnipMulaAtNionIPULATION

(b) < l a t e x i t s h a 1 _ b a s e 6 4 = " c B f 7 Q n 6 Y b k K I P 8 M i v M m r J 7 4 h T 5 M = " > A A A C B 3 i c b V C 7 T g J B F J 3 F F + J r 1 d L C i c Q E C 8 k u j d q R 2 J j Y Y B Q h A U J m h 7 s w Y X Z m M 3 P X h B B K G 3 / F x k K N r b 9 g 5 9 + 4 P A o F T 3 V y z r 2 5 5 5 4 g l s K i 5 3 0 7 m a X l l d W 1 7 H p u Y 3 N r e 8 f d 3 b u 3 O j E c q l x L b e o B s y C F g i o K l F C P D b A o k F A L + p d j v / Y A x g q t 7 n A Q Q y t i X S V C w R m m U t s 9 L A Q n 9 D b h H K y l h i H Q U B t 6 r T S e 4 k C o b t v N e 0 V v A r p I / B n J k x k q b f e r 2 d E 8 i U A h l 8 z a h u / F 2 B o y g 4 J L G O W a i Y W Y 8 T 7 r Q i O l i k V g W 8 P J I y N 6 n C q d S Y J Q K 6 Q T 9 f f G k E X W D q I g n Y w Y 9 u y 8 N x b / 8 x o J h u e t o V B x g q D 4 9 F C Y S I q a j l u h H W G A o x y k h H E j 0 q y U 9 5 h h H N P u c m k J / v z L i 6 R a K l 4 U / Z t S v u z N 2 s i S A 3 J E C s Q n Z 6 R M r k i F V A k n j + S Z v J I 3 5 8 l 5 c d 6 d j + l o x p n t 7 J M / c D 5 / A J G G m J Q = < / l a t e x i t >

Success

rate

for

Knot-tying

Figure 3: GSP trained using Mfoarnwipaurladtiocnonofsinsotnenricgyidlaonsdsdseifgornmiﬁabcleanotbljyecotsu, etp. ge. ,rrfooprem, iss athcehabllaesnegilningepsroabtletmhein robotics.

Even humans learn complex rope manipulation such as tying knots, either by observing an expert

task of (a) manipulating roppeerifnortmo i‘tSo’r bsyharepceeivaisngmexepalsicuitreindstrbuyctioTnPs. ST-Ro PteMst wehrerthoerr aounrdag(ebn)t ckonuoldtmtyainnipgulate ropes

where we report success ratebwy istihmpbloy oobtssterravipngstaahnudmaarnd, wdeevuisaettihoend. ata collected by Nair et al. (2017), where a Baxter robot

manipulated a rope kept on the table in front of it. During exploration, the robot interacts with the

rope by using a pick and place primitive that chooses a random point on the rope, and displaces it by

a randomly chosen length and direction. This process is repeated number of times to collect about

perform it or by receiving ex6p0lKiciintteirnasctirouncptaioirns so. f tWheefotremst(xwt,haet,txhte+r1)otuhrataagreeunstedcotoutlrdainmthaenGipSuPl. ate ropes

by simply observing a humaDnurpinegrfionrfemrenicte. , oWurepurospeostehdeapdpartoaacchoisllteacskteeddtobyfolNloawira evitsuaall. d(e2m0o1n7st)r,atwionheprroevided by a

a Baxter robot manipulated haumroapneexkperpt tfoor nmatnhipeultaatbinlgethine rofrpoeninttooafciotm. pDlexu‘rSin’ sghaepxepanlodrtaytiniogna,knthote. Orourbaogtent, Baxter

interacts with the rope by usirnobgoat, poniclykgaentsdtopolabcseervpertihme iitmivaegethseaqtucenhcoeoosfeisntaermraenddiaotemstaptoesi, natsohnumthaen mroapneipulates the

rope, without any access to the corresponding actions. Note that the knot shape is never encountered

and displaces it by a randomdluyrincghothseesnelflesnupgetrhvisaendddadtairceocllteicotnio. n Tphhaisse apnrdotcheersesfoirse rtheepleeaartneedd Ga SnPummobdeelrwoofuld have to

times to collect about 60K ingteernaercatliiozentopabeirasboleftothfeollfoowrmthe( hxutm, aant,dxemt+on1s)trtahtiaont . aMreoruesdeedtatilos ftorallionwtihnethGe sSuPpp. lementary

material, Section A.

During inference, our proposed approach is tasked to follow a visual demonstration provided by a

human expert for manipulatinMgetthriecroTphee pinertfooramcaoncme polfetxhe‘Sm’osdhelaipseevaanludatteydinbgy amkeansourti. ngOtuhre angoen-nrtig, iBd arexgtiestrration cost

between the rope state achieved by the robot and the state demonstrated by the human at every step in

robot, only gets to observe tthhee diemmaognsetrasteioqnu. eTnhceemaotfchiinngtecromsteisdimaetaesusrteadteussi, ngasthehuthminapnlatme sapnliinpeurolabtuesst ptohinet matching

rope, without any access to thteecchnoirqruees(pToPnSdRinPgMa) cdteisocrnibse. dNinot(eCthhuai t&thReankgnaroatjasnh, a20p0e3)i.sWnehvilerTePnScRoPuMntperroevdides a good

during the self-supervised damtaetcroicllfeorctmioeansuprhinagspeerafnordmtahnecereffoor rceontshtreuclteinagrntheed‘SG’SshPapme, oitdieslnwotoaunladpphraovpreiattoe metric for

knots because the conﬁguration of the rope in a knot is 3D due to intertwining of the rope, and fails

generalize to be able to followto ﬁthned thhuemcoarrnecdt epmoinotncsotrrraestipoonnd. eMnceosr. eWdee,ttahielrseffoorell,ouwse iancctuhraecysuaps pthleemmeetnritcairnyknot tying

material, Section A.1.

where the completion of successful knot is judged by human veriﬁcation.

Metric The performance ofVitshuealmImoidtaetlioins eWvealcuomatpeadreboyur mapeparosaucrhintogththe eprenvoionusribgeisdt mreetghiosdtroaftivoisnuacl oimstitation that between the rope state achievdeedplboyysthane rinovbeorsteamnoddtehl ewshtiachtetadkeems aosnisntpruatteadpbaiyr tohfecuhruremntaanndatgeovael rimy asgteesptionoutput the desired action to reach goal ( Nair et al. , 2017). We re-implement the baseline and train in our setup the demonstration. The matcfhoirnagfacior csotmispamriseoans. uTrheedcoumsipnagristohneisthpeinrfoprmlaetde asppplliensetor-oapbpulesst apnoditnhte monalytcdhififnergence is that technique ( TPS-RPM) descriwbeeddeipnloy(Cfohrwuaird&-coRnasinstgenacryajlaosns, f2or0o0u3r)a.ppWrohacilhe. TThPeSre-sRulPtsMin Fpirgouvreid2esshoawgthoaotdour method metric for measuring performsiagnnicﬁecafnotlrycoountpsetrrfuorcmtisntghetbhaese‘lSin’e sathtaapskeo, fitmiasninpoultatainngathpeprroopperiinatthee m‘Se’ tsrhiacpefoanrd achieves knots because the conﬁgurataionnacocufratchyeorfo6p0%e iinn caomkpnaoritsoins t3oD36% duaechtioeveindtbeyrttwheibnaisneglinoe. f the rope, and it fails to ﬁnd the correct point correspondences. We, therefore, use success rate as the metric in knot tying where the completion o3f.2a suNcAcVeIGssAfTuIOl NknINotINisDOjuOdRgOeFdFIbCyE hEuNmVIRaOnNvMeEriNﬁTcSation.

Visual Imitation QualitativeAenxaatumrapllwesayotfooiunsrtraugcet natrotrbyoitntgo mtoomveainniapnuilnadtoeorroopfﬁecaereensvhiroonwmnenitniFs itoguasrke 2it. to go near a certain location, such as, a refrigerator or a certain person’s ofﬁce. Instead of using language to We compare our approach toctohmembaandsethlienreobtoht, aitndtheips lwooyrks, awne cinomvemrusneicmatoe dweitlhwthhe ircobhottabkyeesithaesr sihnopwuitngaitpaasiirngle image of current and goal images toofothuetpguoatl, thoreadseeqsuireencdeaocf timioangetsoleraedaincghtothfaeragwoaaylg( oNalsa. iIrnebtotahl. s,ce2n0a1ri7os),. thWe reobroet-is required implement the baseline and trtoaianutionnoomuorusselytudpeteformrianefathier mcoomtorpcaormismoann. dTs oforfumrothvienrgatobltahteegtohael. iWmepuosretdanTcuretleBot2 for of consistency loss, we comnpaavrigeattioonausbinagsaenlionnebotahradtcajumsetraufsoer sseansifnogrRwGaBrdimmagoeds. eFl oarsleaarnriengguthlaerGizSePr, aonfautomated selfsupervised scheme for data collection was devised that required no human supervision. The features. The results in Figurreob3otshcoolwlecttehdant uomubremr oeftthraojedctsoirgiensitﬁhcatacnotnlytaionu2t3p0eKrfiontremrasctitohnes bdaatsae, li. ien. e(xatt, taat,sxkt+1), from of manipulating the rope in thtweo‘ﬂSo’orsshoafpaeacaanddemaicchbiueilvdeinsgainstuotcacl. eWsse rthaetnedoefpl6oy0e%d thine lceaormnepdamroisdoelnontoa 3se6p%arate ﬂoor of achieved by the baseline. a building with substantially different textures and furniture layout for performing visual imitation at test time. The details of robotic setup, data collection and network architecture of GSP are described in supplementary material, Section A. 3.2 NAVIGATION IN INDOOR OFFICE ENVIRONMENTS 7 A natural way to instruct a robot to move in an indoor ofﬁce environment is to ask it to go near a certain location, such as a refrigerator or a someone’s ofﬁce. Instead of using language to command the robot, in this work, we communicate with the robot by either showing it a single image of the goal, or a sequence of images leading to faraway goals. In both scenarios, the robot is required to autonomously determine the motor commands for moving to the goal. We used TurtleBot2 for navigation using an onboard camera for sensing RGB images. For learning the GSP, an automated

7

Published as a conference paper at ICLR 2018

Initial Image

Step-7

Step-14

Step-21

Step-28

Step-35

Step-42

Step-44

Step-51

Final Image (Step-59)

Fixed Goal Image

Figure 4: Visualization of the TurtleBot trajectory to reach a goal image ( right) from the initial image (top-left). Since the initial and goal image have no overlap, the robot ﬁrst explores the environment by turning in place. Once it detects overlap between its current image and goal image ( i.e. step 42 onward), it moves towards the goal. Note that we did not explicitly train the robot to explore and such exploratory behavior naturally emerged from the selfsupervised learning.

Model Name Random Search Inverse Model [Nair et. al. 2017] GSP-NoPrevAction-NoFwdConst GSP-NoFwdConst GSP ( Ours)

Run Id-1 Fail Fail 39 steps 22 steps 119 steps

Run Id-2 Fail Fail 34 steps 22 steps 66 steps

Run Id-3 Fail Fail Fail 39 steps 144 steps

Run Id-4 Fail Fail Fail 48 steps 67 steps

Run Id-5 Fail Fail Fail Fail 51 steps

Run Id-6 Fail Fail Fail Fail Fail

Run Id-7 Fail Fail Fail Fail 100 steps

Run Id-8 Fail Fail Fail Fail Fail

Num Success 0 0 2 4 6

Table 1: Quantitative evaluation of various methods on the task of navigating using a single image of goal in an unseen environment. Each column represents a different run of our system for a different initial/ goal image pair. Our full GSP model takes longer to reach the goal on average given a successful run but reaches the goal successfully at a much higher rate.

self-supervised scheme for data collection was devised that doesn’t require human supervision. The robot collected a number of navigation trajectories from two ﬂoors of a academic building which in total contain 230K interactions data, i.e. (xt, at, xt+1). We then deployed the learned model on a separate ﬂoor of a building with substantially different textures and furniture layout for performing visual imitation at test time. The details of the robotic setup, data collection, and network architecture of GSP are described in supplementary material, Section A.2. 1) Goal Finding We ﬁrst tested if the GSP learned by the TurtleBot can enable it to ﬁnd its way to a goal that is within the same room from just a single image of the goal. To test the extrapolative generalization, we keep the Turtlebot approximately 2030 steps away from the target location in a way that current and goal observations have no overlap as shown in Figure 4. We test the robot in an indoor ofﬁce environment on a different ﬂoor that it has never encountered before. We judge the robot to be successful if it stops close to the goal and failure if it crashed into furniture or does not reach the goal within 200 steps. Since the initial and goal images have no overlap, classical techniques such as structure from motion that rely on feature matching cannot be used to infer the executed action. Therefore, in order to reach the goal, the robot must explore its surroundings. We ﬁnd that our GSP model outperforms the baseline models in reaching the target location. Our model learns the exploratory behavior of rotating in place until it encounters an overlap between its current and goal image. Results are shown in Table 1 and videos are available at the website 1. 2) Visual Imitation In the previous paragraph, we saw that the robot can reach a goal that’s within the same room. However, our agent is unable to reach far away goals such as in other rooms using just a single image. In such scenarios, an expert might communicate instructions like go to the door, turn right, go to the closest chair etc. Instead of language instruction, in our setup we provide a sequence of landmark images to convey the same highlevel idea. These landmark images were captured from the robot’s camera as the expert moved the robot from the start to a goal location. However, note that it is not necessary for the expert to control the robot to capture the images because we don’t make use of the expert’s actions, but only the images. Instead of providing the image after every action in the demonstration, we only provided every ﬁfth image. The rationale behind this choice is that we want to sample the demonstration sparsely to minimize the agent’s 1https: //pathak22. github. io/zeroshot-imitation/ 8

Published as a conference paper at ICLR 2018

Demo Image-1

Demo Image-2

Demo Image-3

Demo Image-4

Demo Image-5

Demo Image-6

Initial Robot Image

Robot WayPoint-1

Robot WayPoint-2

Robot WayPoint-3 Robot WayPoint-4

Robot WayPoint-5

Robot WayPoint-6

Figure 5: The performance of TurtleBot at following a visual demonstration given as a sequence of images ( top row). The TurtleBot is positioned in a manner such that the ﬁrst image in demonstration has no overlap with its current observation. Even under this condition the robot is able to move close to the ﬁrst demo image ( shown as Robot WayPoint-1) and then follow the provided demonstration until the end. This also exempliﬁes a failure case for classical methods; there are no possible keypoint matches between WayPoint1 and WayPoint-2, and the initial observation is even farther from WayPoint1.

Model Name SIFT GSP-NoPrevAction-NoFwdConst GSP-NoFwdConst GSP ( ours)

Maze Demonstration Run-1 Run-2 Run-3 10% 5% 15% 60% 70% 100% 65% 90% 100% 100% 60% 100%

Loop Demonstration Run-1 Run-2 Run-3

—

—

—

—

—

—

0% 0% 0%

0% 100% 100%

Table 2: Quantitative evaluation of TurtleBot’s performance at following visual demonstrations in two scenarios: maze and the loop. We report the % of landmarks reached by the agent across three runs of two different demonstrations. Results show that our method outperforms the baselines. Note that 3 more trials of the loop demonstration were tested under signiﬁcantly different lighting conditions and neither model succeeded. Detailed results are available in the supplementary materials.

reliance on the expert. Such sub-sampling (as shown in Figure 5) provides an easy way to vary the complexity of the task. We evaluate via multiple runs of two demonstrations, namely, maze demonstration where the robot is supposed to navigate through a mazelike path and perturbed loop demonstration, where the robot is supposed to make a complete loop as instructed by demonstration images. The loop demonstration is longer and more difﬁcult than the maze. We start the agent from different starting locations and orientations with respect to that of demonstration. Each orientation is initialized such that no part of the demonstration’s initial frame is visible. Results are shown in Table 2. When we sample every frame, our method and classical structure from motion can both be used to follow the demonstration. However, at sub-sampling rate of ﬁve, SIFT-based feature matching approaches did not work and ORBSLAM2 ( Mur-Artal & Tardo´s, 2017) failed to generate a map, whereas our method was successful. Notice that providing sparse landmark images instead of dense video adds robustness to the visual imitation task. In particular, consider the scenario in which the environment has changed since the time the demonstration was recorded. By not requiring the agent to match every demonstration image frameby-frame, it becomes less sensitive to changes in the environment. 3. 3 3D NAVIGATION IN VIZDOOM We have evaluated our approach on realrobot scenarios thus far. To further analyze the performance and robustness of our approach through large scale experiments, we setup the same navigation task as described in previous subsection in a simulated VizDoom environment. Our goal is to measure: (1) the robustness of each method with proper error bars, (2) the role of initial self-supervised data collection for performance on visual imitation, (3) the quantitative difference in modeling forward consistency loss in feature space in comparison to raw visual space. 9

Published as a conference paper at ICLR 2018

Model Name

Same Map, Same Texture Median % Efﬁciency %

Random Exploration for Data Collection:

GSP-NoFwdConst GSP (ours pixels) GSP (ours features)

63. 2 ± 5. 7 62. 2 ± 5. 1 68. 9 ± 6. 9

36. 4 ± 3. 3 43. 0 ± 2. 6 53. 9 ± 4. 0

Curiosity-driven Exploration for Data Collection:

GSP-NoFwdConst GSP-FwdRegularizer GSP (ours pixels) GSP (ours features)

78. 2 ± 2. 3 78. 4 ± 3. 4 78. 2 ± 3. 4 78. 2 ± 4. 6

63. 0 ± 4. 3 59. 8 ± 4. 1 65. 2 ± 4. 2 67. 0 ± 3. 3

Same Map, Diff Texture Median % Efﬁciency %

32. 2 ± 0. 7 32. 4 ± 0. 8 32. 4 ± 0. 7

28. 9 ± 4. 0 30. 9 ± 2. 9 47. 4 ± 7. 6

43. 2 ± 2. 6 50. 6 ± 4. 7 47. 1 ± 4. 7 49. 4 ± 4. 8

33. 9 ± 3. 0 30. 9 ± 3. 0 32. 4 ± 3. 0 26. 9 ± 1. 5

Diff Map, Diff Texture Median % Efﬁciency %

34. 5 ± 0. 6 35. 4 ± 1. 1 39. 1 ± 2. 0

23. 1 ± 2. 4 29. 3 ± 3. 9 30. 4 ± 2. 5

40. 2 ± 4. 0 37. 9 ± 1. 1 44. 8 ± 4. 0 47. 1 ± 3. 0

27. 3 ± 1. 9 28. 9 ± 1. 7 29. 5 ± 1. 9 24. 1 ± 1. 7

Table 3: Quantitative evaluation of our proposed GSP and the baseline models at following visual demonstrations in VizDoom 3D Navigation. Medians and 95% conﬁdence intervals are reported for demonstration completion and efﬁciency over 50 seeds and 5 human paths per environment type.

In VizDoom, we collect data by deploying two types of exploration methods: random exploration and curiosity-driven exploration ( Pathak et al. , 2017). The hypothesis is that if the initial data collected by the robot is driven by a better strategy than just random, this should eventually help the agent follow long demonstrations better. Our environment consists of 2 maps in total. We train on one map with 5 different starting positions for collecting exploration data. For validation, we collect 5 human demonstrations in a map with the same layout as in training but with different textures. For zero-shot generalization, we collect 5 human demonstrations in a novel map layout with novel textures. Exact details for data collection and training setup are in the supplementary, Section A.3. Metric We report the median of maximum distance reached by the robot in following the given sequence of demonstration images. The maximum distance reached is the distance of farthest landmark point that the agent reaches contiguously, i.e., without missing any intermediate landmarks. Measuring the farthest landmark reached does not capture how efﬁciently it is reached. Hence, we further measure efﬁciency of the agent as the ratio of number of steps taken by the agent to reach farthest contiguous landmark with respect to the number of steps shown in human demonstrations. Visual Imitation The task here is same as the one in real robot navigation where the agent is shown a sparse sequence of images to imitate. The results are in Table 3. We found that the exploration data collected via curiosity signiﬁcantly improves the ﬁnal imitation performance across all methods including the baselines with respect to random exploration. Our baseline GSP model with a forward regularizer instead of consistency loss ends up overﬁtting to the training layout. In contrast, our forward-consistent GSP model outperforms other methods in generalizing to new map with novel textures. This indicates that the forward consistency is possibly doing more than just regularizing the policy features. Training forward consistency loss in feature space further enhances the generalization even when both pixel and feature space models perform similarly on training environment. 4 RELATED WORK Our work is closely related to imitation learning, but we address a different problem statement that gives less supervision and requires generalization across tasks during inference. Imitation Learning The two main threads of imitation learning are behavioral cloning ( Argall et al. , 2009; Pomerleau, 1989), which directly supervises the mapping of states to actions, and inverse reinforcement learning (Abbeel & Ng, 2004; Ho & Ermon, 2016; Levine et al. , 2016; Ng & Russell, 2000; Ziebart et al. , 2008), which recovers a reward function that makes the demonstration optimal ( or nearly optimal). Inverse RL is most commonly achieved with state-actions, and is difﬁcult to extend to ﬁtting the reward to observations alone, though in principle state occupancy could be sufﬁcient. Recent work in imitation learning (Duan et al. , 2017; Finn et al. , 2017; Gupta et al. , 2017) can generalize to novel goals, but require a wealth of demonstrations comprised of expert stateactions for learning. Our approach does not require expert actions at all. Visual Demonstration The common scenario in LfD is to assume full knowledge of expert states and actions during demonstrations, but several papers have focused on relaxing this supervision to 10

Published as a conference paper at ICLR 2018 visual observations alone. Nair et al. (2017) observe a sequence of images from the expert demonstration for performing rope manipulations. Sermanet et al. (2017; 2018) imitate humans with robots by self-supervised learning but require expert supervision at training time. Third person imitation learning (Stadie et al. , 2017) and the concurrent work of imitation-fromobservation (Liu et al. , 2018) learn to translate expert observations into agent observations such that they can do policy optimization to minimize the distance between the agent trajectory and the translated demonstration, but they require demonstrations for learning. Visual servoing is a standard problem in robotics ( Koichi & Tom, 1993) that seeks to take actions that align the agent’s observation with a target conﬁguration of carefullydesigned visual features (Wilson et al. , 1996; Yoshimi & Allen, 1994) or raw pixel intensities ( Caron et al. , 2013). Classical methods rely on ﬁxed features or policies, but more recently end-to-end learning has improved results ( Lampe & Riedmiller, 2013; Lee et al. , 2017). Forward/Inverse Dynamics and Consistency Numerous prior works, such as Ebert et al. (2017); Oh et al. (2015); Watter et al. (2015), have learned forward dynamics model for planning actions. The works of Agrawal et al. (2016); Jordan & Rumelhart (1992); Pathak et al. (2017); Wolpert et al. (1995) jointly learn forward and inverse dynamics model but do not optimize for consistency between the forward and inverse dynamics. We empirically show that learning models by our forward consistency loss signiﬁcantly improves task performance. Enforcing consistency as a meta-supervision has also been successful in ﬁnding visual correspondences ( Zhou et al. , 2016) or unpaired image translations (Zhu et al. , 2017). Goal Conditioning By parameterizing the value or policy function with a goal, an agent can learn and do multiple tasks. The idea of learning goal-conditioned policies has been explored in ( Agrawal et al. , 2016; Andrychowicz et al. , 2017; Nair et al. , 2017; Schaul et al. , 2015). Similarly to hindsight experience replay (Andrychowicz et al. , 2017) we draw goals from experience, but our policy optimization has better sample efﬁciency through supervised learning and dynamics modeling instead of reinforcement learning. Moreover, we work from high-dimensional visual inputs instead of knowledge of the true states and do not make use of a task reward during training. In our setting, all of the expert goals are followed zeroshot since they are only revealed after learning. 5 DISCUSSION In this work, we presented a method for imitating expert demonstrations from visual observations alone. In contrast to most work in imitation learning, we never require access to expert actions. The key idea is to learn a GSP using data collected by selfsupervised exploration. However, this limits the quality of the learned GSP as per the exploration data. For instance, we deploy random exploration on our realworld navigation robot, which means that it would almost never follow trajectories that go between rooms. Consequently, the learned GSP is unable to navigate towards a goal image taken in another room without requiring intermediate subgoals. Pathak et al. (2017) show that the agent learns to move along corridors and transition between rooms purely driven by curiosity in VizDoom. Training GSP on such a structured data could equip the agent with more interesting search behaviors, e.g., going across rooms to ﬁnd a goal. In general, using better methods of exploration for training the GSP could be a fruitful direction toward generalizing zeroshot imitation. One limitation of our approach is that we require ﬁrstperson view demonstrations. Extension to third-person demonstrations (Liu et al. , 2018; Stadie et al. , 2017) would make the method applicable in more general scenarios. Another limitation is that, in the current framework, it is implicitly assumed that the statistics of visual observations when the expert demonstrates the task and the agent follows it are similar. For e.g., when the expert performs a demonstration in one setting, say in daylight and the agent needs to imitate say in the evening, the change in the lighting conditions might result in worse performance. Making the GSP robust to such nuisance changes or other changes in environment by domain adaptation would be necessary to scale the method to practical problems. Another thing to note is that, in the current framework, we do not learn from expert demonstrations, but simply imitate them. It would be interesting to investigate ways for an agent to learn from the expert to bias its exploration to more useful parts of the environment. While we used a sequence of images to provide a demonstration, our work makes no image-speciﬁc assumptions and can be extended to using formal language for communicating goals. For instance, after training the GSP, instead of transforming an image into features φ as described in section 2. 2, one could possibly learn a mapping to transform language instructions into this feature space. 11

Published as a conference paper at ICLR 2018 ACKNOWLEDGMENTS We would like to thank members of BAIR for fruitful discussions and comments. This work was supported in part by DARPA; NSF IIS1212798, IIS-1427425, IIS-1536003, Berkeley DeepDrive, and an equipment grant from NVIDIA and the Valrhona Reinforcement Learning Fellowship. DP is supported by NVIDIA and Snapchat’s graduate fellowships. REFERENCES Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In ICML, 2004. 10 Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke by poking: Experiential learning of intuitive physics. NIPS, 2016. 2, 4, 11 Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In NIPS, 2017. 2, 11 Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from demonstration. Robotics and autonomous systems, 2009. 1, 10 Albert Bandura and Richard H Walters. Social learning theory, volume 1. Prentice-hall Englewood Cliffs, NJ, 1977. 1 Cynthia Breazeal and Brian Scassellati. Robots that imitate humans. Trends in cognitive sciences, 2002. 1 Guillaume Caron, Eric Marchand, and El Mustapha Mouaddib. Photometric visual servoing for omnidirectional cameras. Autonomous Robots, 35(2-3):177–193, 2013. 11 Haili Chui and Anand Rangarajan. A new point matching algorithm for non-rigid registration. CVIU, 2003. 7 Andrew J Davison and David W Murray. Mobile robot localisation using active vision. In ECCV, 1998. 6 Ru¨diger Dillmann. Teaching and learning of robot tasks via observation of human performance. Robotics and Autonomous Systems, 2004. 1 Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. In NIPS, 2017. 2, 10 Frederik Ebert, Chelsea Finn, Alex X Lee, and Sergey Levine. Self-supervised visual planning with temporal skip connections. arXiv preprint arXiv:1710. 05268, 2017. 11 Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot visual imitation learning via meta-learning. CoRL, 2017. 2, 10 David Foster and Peter Dayan. Structure in the space of value functions. Machine Learning, 2002. 3 Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, and Jitendra Malik. Cognitive mapping and planning for visual navigation. CVPR, 2017. 10 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 15 Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In NIPS, 2016. 10 Katsushi Ikeuchi and Takashi Suehiro. Toward an assembly plan from observation. i. task recogni- tion with polyhedral objects. IEEE Transactions on Robotics and Automation, 1994. 1 12

Published as a conference paper at ICLR 2018

Michael I Jordan and David E Rumelhart. Forward models: Supervised learning with a distal teacher. Cognitive science, 1992. 3, 11

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015. 15

Diederik P Kingma and Max Welling. arXiv:1312. 6114, 2013. 4

Auto-encoding variational bayes.

arXiv preprint

Hashimoto Koichi and Husband Tom. Visual servoing: real-time control of robot manipulators based on visual sensory feedback, volume 7. World scientiﬁc, 1993. 11

Yasuo Kuniyoshi, Masayuki Inaba, and Hirockika Inoue. Teaching by showing: Generating robot programs by visual observation of human performance. In International Symposium on Industrial Robots, 1989. 1

Yasuo Kuniyoshi, Masayuki Inaba, and Hirochika Inoue. Learning by watching: Extracting reusable task knowledge from visual observation of human performance. IEEE Transactions on Robotics and Automation, 1994. 1

Thomas Lampe and Martin Riedmiller. Acquiring visual servoing reaching and grasping skills using neural reinforcement learning. In Neural Networks (IJCNN), The 2013 International Joint Conference on, pp. 1–8. IEEE, 2013. 11

Alex X Lee, Sergey Levine, and Pieter Abbeel. Learning visual servoing with deep features and ﬁtted q-iteration. arXiv preprint arXiv:1703. 11000, 2017. 11

Sergey Levine, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen. Learning hand-eye coordination for robotic grasping with largescale data collection. In ISER, 2016. 2, 10

YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation: Learning to imitate behaviors from raw video via context translation. ICRA, 2018. 11

Mapillary. Open source structure from motion pipeline. https://github. com/mapillary/ OpenSfM , 2016. 6

Raul Mur-Artal and Juan D Tardo´s. Orb-slam2: An opensource slam system for monocular, stereo, and rgbd cameras. IEEE Transactions on Robotics, 2017. 6, 9

Ashvin Nair, Dian Chen, Pulkit Agrawal, Phillip Isola, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Combining self-supervised learning and imitation for visionbased rope manipulation. ICRA, 2017. 2, 6, 7, 11, 15

Andrew Y Ng and Stuart J Russell. Algorithms for inverse reinforcement learning. In ICML, pp. 663–670, 2000. 1, 10

Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. In NIPS, 2015. 11

Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems for autonomous mental development. Evolutionary Computation, 2007. 3

Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In ICML, 2017. 3, 4, 10, 11, 16

Lerrel Pinto and Abhinav Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. ICRA, 2016. 2

Dean A Pomerleau. ALVINN: An autonomous land vehicle in a neural network. In NIPS, 1989. 1, 10

Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401. 4082, 2014. 4

Stefan Schaal. Is imitation learning the route to humanoid robots? Trends in cognitive sciences, 1999. 1

13

Published as a conference paper at ICLR 2018 Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In ICML, 2015. 3, 11 Jurgen Schmidhuber. A possibility for implementing curiosity and boredom in modelbuilding neural controllers. In From animals to animats: Proceedings of the ﬁrst international conference on simulation of adaptive behavior, 1991. 3 Pierre Sermanet, Kelvin Xu, and Sergey Levine. Unsupervised perceptual rewards for imitation learning. In RSS, 2017. 11 Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, and Sergey Levine. Time-contrastive networks: Self-supervised learning from video. In ICRA, 2018. 5, 11 Bradly C. Stadie, Pieter Abbeel, and Ilya Sutskever. Third-person imitation learning. In ICLR, 2017. 11 Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In NIPS, 2015. 11 William J Wilson, CC Williams Hulls, and Graham S Bell. Relative end-effector control using cartesian position based visual servoing. IEEE Transactions on Robotics and Automation, 12( 5): 684–696, 1996. 11 Daniel M Wolpert, Zoubin Ghahramani, and Michael I Jordan. An internal model for sensorimotor integration. Science-AAAS-Weekly Paper Edition, 1995. 11 Yezhou Yang, Yi Li, Cornelia Fermu¨ller, and Yiannis Aloimonos. Robot learning manipulation action plans by ”watching” unconstrained videos from the world wide web. In AAAI, 2015. 1 Billibon H Yoshimi and Peter K Allen. Active, uncalibrated visual servoing. In Robotics and Automation, 1994. Proceedings. , 1994 IEEE International Conference on, pp. 156–161. IEEE, 1994. 11 Tinghui Zhou, Philipp Krahenbuhl, Mathieu Aubry, Qixing Huang, and Alexei A. Efros. Learning dense correspondence via 3d-guided cycle consistency. In CVPR, 2016. 11 Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. ICCV, 2017. 11 Brian D. Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In AAAI, 2008. 10 14

Published as a conference paper at ICLR 2018 A SUPPLEMENTARY MATERIAL We evaluated our proposed approach across number of environments and tasks. In this section, we provide additional details about the experimental task setup and hyperparameters. A.1 ROPE MANIPUATION Robotic Setup Our setup of Baxter robot for rope manipulation task follows the one described in Nair et al. (2017). We re-use the data that is collected by a Baxter robot interacting with a rope kept on a table in front of it in a selfsupervised manner, and consists of approximately 60K interaction pairs. Implementation Details The base architecture for all the methods consists of a pretrained AlexNet, whose features are fed into a skill policy network that predicts the location of grasp, direction of displacement, and the magnitude of displacement. For the forward regularizer baseline, a forward model is trained to jointly regularize the AlexNet features along with the skill policy network with loss weight of forward model set to 0. 1. For our proposed forward-consistent GSP, a forward consistency loss is then applied to the actions predicted by the skill policy network. The forward consistency loss weight is set to 0. 1. Since this is a fully observed setup, we did not use recurrence in any of the skill policy networks. All the models are optimized using Adam (Kingma & Ba, 2015) with a learning rate of 1e − 4. For the ﬁrst 40K iterations, the AlexNet weights were frozen, and then ﬁne-tuned jointly with the later layers. A.2 NAVIGATION IN INDOOR OFFICE ENVIRONMENTS Robotic Setup We used the TurtleBot2 robot comprising of a wheeled Kobuki base and an Orbbec Astra camera for capturing RGB images for all our experiments. The robot’s action space had four discrete actions: move forward, turn left, turn right, and stand still ( i.e., no-op). The forward action is approximately 10cm forward translation and the turning actions are approximately 1418 degrees of rotation. These numbers vary due to the use of velocity control. A powerful on-board laptop was used to process the images and infer the motor commands. Several modiﬁcations were made to the default TurtleBot setup: the base’s batteries were replaced with longer lasting ones, and the default NVIDIA Jetson TK1 embedded board was replaced with a more powerful GigaByte Aero laptop and an accompanying portable charging power bank. Self-supervised Data Collection We devised an automated selfsupervised scheme for data collection which does not require any human supervision. In our scheme, the robot ﬁrst samples one out of four actions and then the number of times to repeat the selected action ( i.e. action repeat). The no-op action is sampled with probability 0. 05 and the other three actions are sampled with equal probability. In case the no-op action is chosen, an action repeat of { 1, 2} steps is uniformly sampled. In case of other actions, an action repeat of 1-5 steps is randomly and uniformly chosen. The robot autonomously repeated this process and collected 230K interactions from two ﬂoors of an academic building. If the robot crashes into an object, it performs a reset maneuver by ﬁrst moving backwards and then turning right/ left by a uniformly sampled angle between 90-270 degrees. A separate ﬂoor of the building with substantially different furniture layout and visual textures is then used for testing the learned model. Implementation Details The data collected by selfsupervised exploration is then used to train our recurrent forwardconsistent GSP. The base architecture of our model is an ImageNet pretrained ResNet-50 (He et al. , 2016) network. Input are the images and output are the actions of robot. The forward consistency model is ﬁrst pre-trained and then ﬁnetuned together end-to-end with the GSP. The loss weight of the forward model is 0. 1, and the objective is minimized using Adam (Kingma & Ba, 2015) with learning rate of 5e − 4. A.3 3D NAVIGATION IN VIZDOOM Selfsupervised Data Collection Our environment consists of two map. One map is used for training and validation, with different textures for validation. Second map has different textures than training and validation and is used for generalization experiments. For both curiosity and random exploration, we collect a total of 1. 5 million frames each with action repeat of 4 collected in the 15

Published as a conference paper at ICLR 2018

Model Name SIFT GSP-NoPrevAction-NoFwdConst GSP-NoFwdConst GSP ( ours)

Maze Runs - Optimal Steps: 100

Run-1

Run-2

Run-3

2/20 (10) 12/20 (109) 13/20 (147)

1/20 (9) 14/20 (184) 18/20 (325)

3/20 (38) 20/20 (263) 20/20 (166)

20/20 (353) 12/20 (194) 20/20 (168)

Loop Runs - Optimal Steps: 85

Run-1

Run-2

Run-3

— — 0/17 (0)

— — 0/17 (0)

— — 0/17 (0)

0/17 (0) 17/17 (243) 17/17 (165)

Table 4: Quantitative evaluation of TurtleBot’s performance at following visual demonstrations in two conditions: maze and the loop. The fraction denotes how many landmarks it reaches out of the total number of landmarks in the full demonstration. The bracketed number represents the number of actions the agent took to reach its farthest landmark.

Model Name

Same Map, Same Texture Mean % Efﬁciency %

Random Exploration for Data Collection:

GSP-NoFwdConst GSP (ours pixels) GSP (ours features)

61. 8 ± 0. 9 61. 0 ± 1. 0 62. 0 ± 1. 0

60. 4 ± 2. 1 68. 0 ± 2. 2 75. 8 ± 2. 5

Curiosity-driven Exploration for Data Collection:

GSP-NoFwdConst GSP-FwdRegularizer GSP (ours pixels) GSP (ours features)

70. 7 ± 0. 9 70. 6 ± 0. 9 71. 0 ± 0. 9 68. 8 ± 1. 0

66. 9 ± 1. 4 67. 9 ± 1. 6 73. 1 ± 2. 7 72. 0 ± 1. 7

Same Map, Diff Texture Mean % Efﬁciency %

37. 6 ± 0. 7 38. 1 ± 0. 7 37. 0 ± 0. 7

68. 6 ± 2. 5 69. 1 ± 2. 5 87. 1 ± 2. 8

49. 8 ± 0. 8 51. 9 ± 0. 8 53. 3 ± 0. 9 53. 2 ± 0. 8

55. 8 ± 2. 2 49. 3 ± 1. 6 53. 4 ± 2. 0 53. 0 ± 2. 3

Diff Map, Diff Texture Mean % Efﬁciency %

42. 2 ± 0. 8 40. 3 ± 0. 9 48. 7 ± 0. 9

50. 6 ± 1. 9 64. 2 ± 2. 3 52. 5 ± 1. 8

51. 2 ± 1. 0 48. 3 ± 1. 0 52. 2 ± 1. 0 52. 8 ± 0. 9

39. 5 ± 1. 3 49. 3 ± 1. 8 44. 0 ± 1. 5 37. 7 ± 1. 3

Table 5: Quantitative evaluation of our proposed GSP and the baseline models at following visual demonstrations in VizDoom 3D Navigation. Means and standard errors are reported for demonstration completion and efﬁciency over 50 seeds and 5 human paths per environment type.

standard

DoomMyWayHome

map

used

for

training

in

Pathak

et

al.

(2017).

∼

2 3

of

the

data

comes

from random-room resets,

and ∼

1 3

of the data comes from a ﬁxed-room reset (i.e,

room number

10). The curiosity policy was half sampled and half greedy with the exact split being 40% greedy

policy random-room reset, 25% sample policy randomroom reset, 25% sample policy ﬁxed-room

reset, and 10% greedy policy ﬁxed-room reset.

For each scenario, we collect 5 human demonstrations each and give every 10th frame as input to the agent for the task of visual imitation. For each human path, we evaluate on 50 different seeds where the agent starts with a uniformly sampled orientation. We then get the median across 250 (50x5) total runs for each type of environment and report median of the percentage of the human path reached by the agent and how soon it got to that point relative to the human.

In the main paper, we report median accuracy and the conﬁdence interval for median 2. Since the initial position of the agent is randomized in orientation compared to the one in visual demonstration, the mean results suffer from high variance due to outliers. Hence, median accuracy results in a more reliable metric. However, we report mean results in Table 5 for the completion.

Implementation Details All models were trained with batch size 64, Adam Solver with 1e-4 learning rate, and landmark slices uniformly sampled between 5 to 15 action steps for each batch. The observations are 42x42 resolution, grayscale images with only onetime channel both for goal and current state. All models used the same goal recognizer that was trained on the curiosity data. For selecting the hyper-parameters in forward regularizer, pixel-based forward consistency, and featurebased forward consistency models, we selected the best loss coefﬁcient among {0. 01, 0. 05, 0. 1} that achieved the highest median completion on our validation environment which consisted of the training maps with novel textures.

2Formula for computing median conﬁdence intervals: http://www. ucl. ac. uk/ich/ short-courses-events/about-stats-courses/statsrm/Chapter_8_Content/ confidence_interval_single_median 16

