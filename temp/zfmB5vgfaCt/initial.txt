Under review as a conference paper at ICLR 2022 TR A NSL O WDO W N : EFFICIENCY ATTACKS ON NEURAL MACHINE TRANSLATION SYSTEMS Anonymous authors Paper under doubleblind review 
ABSTRACT Neural machine translation ( NMT) systems have received massive attention from academia and industry. Despite a rich set of work focusing on improving NMT systems’ accuracy, the less explored topic of efﬁciency is also important to NMT systems because of the realtime demand of translation applications. In this paper, we observe an inherent property of NMT system, that is, NMT systems’ efﬁciency is related to the output length instead of the input length. Such property results in a new attack surface of NMT system—an adversary can slightly changing inputs to incur signiﬁcant amount of redundant computations in NMT systems. Such abuse of NMT systems’ computational resources is analogous to the denialofservice attacks. Abuse of NMT systems’ computing resources will affect the service quality ( e.g., prolong response to users’ translation requests) and even make the translation service unavailable ( e.g., running out resource such as batteries of mobile devices) . To further the understanding on such efﬁciency-oriented threats and raise community’s concern on the efﬁciency robustness of NMT systems, we propose a new attack approach, TranSlowDown, to test the efﬁciency robustness of NMT systems. To demonstrate the effectiveness of TranSlowDown, we conduct a systematic evaluation on three publicavailable NMT systems: Google T5, Facebook Fairseq, and Helsinki-NLP translators. The experimental results show that TranSlowDown can increase NMT systems’ response latency up to 1232% and 1056% on Intel CPU and Nvidia GPU respectively by inserting only three characters into existing input sentences. Our results also show that the adversarial examples generated by TranSlowDown can consume more than 30 times battery power than the original benign example. Such results suggest that further research is required for protecting NMT systems against efﬁciencyoriented threats. 1 INTRODUCTION Recently, Neural Machine Translation ( NMT) systems have received massive attention from academia and industry ( Bahdanau et al. , 2015; Kalchbrenner et al. , 2016; Vaswani et al. , 2017; Belinkov & Bisk, 2018). NMT systems overcome many weaknesses of traditional phrasebased translation models and can capture long dependencies in sentences; thus, they are widely used in commercial translation systems. For example, Microsoft has deployed the NMT systems in many commercial products since 2016 ( Hassan Awadalla et al. , 2017; 2018a;b; Gu et al. , 2018); Google Translate claims to have translated over 100 billion words daily in 109 languages ( Turovsky, 2016; Caswell & Liang, 2020; Pitman, 2021) . For NMT systems, efﬁciency is critical because of translation applications’ realtime demand (Tu et al. , 2017; Guo et al. , 2019; Xia et al. , 2019). However, it is unknown whether NMT systems are robust against efﬁciencyoriented adversarial pressure. Despite a rich set of works (Cheng et al. , 2020; Jin et al. , 2020; Belinkov & Bisk, 2017; Cheng et al. , 2019; Wu et al. , 2018; Yang et al. , 2017) evaluate NMT systems’ accuracy robustness through maximizing the errors, understanding the NMT systems’ efﬁciency robustness has not received much attention. In order to study NMT systems’ efﬁciency robustness, we ﬁrst need to ﬁgure out what factors will affect NMT’s efﬁciency. In this paper, we observe a natural property of NMT systems, i.e.,, NMT’s computational consumption is volatile for different inputs because NMT systems invoke the underlying decoder with nondeterministic numbers of iterations to generate output tokens ( Vaswani et al. , 1

 2017; Liu et al. , 2020). This property exposes a new vulnerability of NMT systems, an adversary can design speciﬁc inputs to cause enormous computation overhead in NMT systems, thus wasting the computational resources of NMT systems. Based on such observation, in this paper, we investigate: Can adversary make slight modiﬁcation on textual inputs to increase NMT systems’ computational consumption and decrease NMT systems’ efﬁciency? If so, how severe the efﬁciency degradation can be? New Vulnerability. We consider a new attack surface of NMT models, analogous to the vulnerabilities leading to the denial of service ( DoS) attacks (Zhang et al. , 2015; Qin et al. , 2018) that have plagued the security committee for decades. Speciﬁcally, the adversary’s goal is to decrease the victim NMT system’s efﬁciency ( i.e., response latency, energy consumption) with unnoticeable perturbations on the inputs to the system. This attack will result in devastating consequences for many realworld applications. For example, abusing computational resources on commercial machine translation service providers ( e.g., Huggingface (Wolf et al. , 2020)). An efﬁciency attack will cause enormous redundant computational resources and affect the service quality of benign users. Furthermore, abusing computational resources on mobile devices or IoT devices might shorten the battery charge cycle and result in the unavailability of the devices. New Attack. In this paper, we evaluate NMT systems’ efﬁciency robustness by generating adversarial inputs that consume much greater amount of computation resources than normal inputs on NMT systems. Speciﬁcally, we propose TranSlowDown, which is based on the observation that the source of vulnerability is that NMT systems’ efﬁciency is related to the output length instead of the input length. Speciﬁcally, NMT systems iteratively compute the output token until the systems generate the particular end of sentence ( EOS) token. Thus, we design a novel algorithm to search for a minimal perturbation to delay the appearance of EOS. Speciﬁcally, TranSlowDown can generate both tokenlevel and character-level perturbations. After applying the minimal perturbation on the benign sentences, the probability of EOS of output token will decrease, resulting longer output that costs NMT systems more computational resources. Evaluation. To evaluate the effectiveness of TranSlowDown, we use TranSlowDown to attack three publicavailable NMT systems (i.e., Google T5 (Raffel et al. , 2019), Facebook FairSeq (Liu et al. , 2020), and Helsinki-NLP), and compare TranSlowDown against two accuracybased attack algorithms. We ﬁrst measure the ﬂoating-point operations (FLOPs) and response latency of the victim NMT systems when translating benign and adversarial examples. Then, we apply I-FLOPs and I-Latency (deﬁned in Equation 4) to quantify the severity of efﬁciency degradation caused by generated adversarial examples. The evaluation results show: for the token-level attack, TranSlowDown generate adversarial examples increase victim NMT systems’ FLOPs, CPU latency, GPU latency up to 2131. 92%, 2403. 21%, and 2054. 83% respectively with only perturbing two tokens in sentences; for the character-level attack, TranSlowDown generated adversarial examples increase victim NMT systems’ FLOPs, CPU latency, GPU latency up to 1102. 86%, 1232. 71%, and 1056. 88% respectively with only inserting three characters in sentences. We also form a real-world case study to demonstrate the negative impact of the efﬁciency threat. Speciﬁcally, we deploy Google’s T5 on a mobile device and investigate how adversarial examples affect the mobile device’s battery power consumption. The results show that by inserting only one character into the benign sentence, the adversarial examples generated by TranSlowDown can consume more than 30 times battery power than the original benign example. Our results suggest that further research is required for protecting NMT systems against this emerging security threat. 2 BACKGROUND & RELATED WORK 2. 1 NEURAL MACHINE TRANSLATION SYSTEMS Neural machine translation ( NMT) (Vaswani et al. , 2017; Liu et al. , 2020) systems apply neural networks to approximate the conditional probability P (Y |X), where X = [x1, x2, · · · , xm] is the source token sequences and Y = [y1, y2, · · · , yn] is the target token sequences. As shown in Figure 1, a typical NMT system consists of an encoder fen( ·) and a decoder fde(·). The encoder encodes the source input X into hidden representation H, then the decoder starts from a special token ( SOS), and iteratively accesses H for an auto-regressive generation of each token yi until the end of sequence token ( EOS) is reached. An important observation from the NMT working mechanisms is that NMT will iteratively running the decoder fde to generate a output token until EOS is 2


reached. Thus, if the generated output sequence is longer, the NMT will consume more computational resources and becomes less efﬁcient. In our example, the NMT system run the decoder four times to generate the output.
2. 2 DNN’S EFFICIENCY
Recently, the efﬁciency of deep neural net Input
works (DNNs) has raised a huge concern because
SOS I
Like Reading
of their substantial inference-time computational
costs. To reduce DNN’ inference-time compu- Encoder
H
Decoder
tational costs and make it feasible to applying
DNNs for real-time applications, many existing
Output
I
Like Reading EOS
work has been proposed. There are two main
1
2
3
4
techniques: The ﬁrst type (Howard et al. (2017);
Zhang et al. (2018)) of the techniques prune the DNNs ofﬂine to identify important neurons and Figure 1: Working mechanism of NMT systems
remove the unimportant neurons. After pruning, the smaller size DNNs could achieve competi tive accuracy with the original DNNs but require less computational costs. Another type of tech niques Wang et al. (2018); Graves (2016); Figurnov et al. (2017), called input-adaptive techniques,
this type of technique dynamically skip a certain part of the DNNs to reduce the number of com putations in the inference time. By skipping some parts of the DNNs, the input-adaptive DNNs
can balance the accuracy and computational costs. However, recent studies (Haque et al. , 2020;
Hong et al. , 2020) show input-adaptive DNNs are not robustness against the adversary attack, which
implies the input-adaptive will not save computational costs under attacks.
2. 3 ADVERSARIAL ATTACKS Existing work on adversarial machine learning has shown that even the stateof-the-art DNNs can be fooled by adversarial examples ( Carlini & Wagner, 2017; Athalye et al. , 2018). Adversarial examples are the elaborately crafted samples that apply humanunnoticeable perturbations on benign samples to maximize the target DNNs’ errors. Based on prior knowledge about victim DNNs, the generation of adversarial examples could be categorized into whitebox and black-box attacks. In the white-box settings (Goodfellow et al. , 2014; Moosavi-Dezfooli et al. , 2016; Kurakin et al. , 2016; Carlini & Wagner, 2017; Jang et al. , 2017; Madry et al. , 2017; Chen et al. , 2018; Rony et al. , 2019), the adversary know the victim DNNs’ architecture and parameters and can compute gradients to generate adversarial examples. In the black-box settings (Brendel et al. , 2017; Chen et al. , 2020; Brendel et al. , 2019), the adversary can also exploit a surrogate model for launching the attack.
3 ATTACK METHODOLOGY 3. 1 THREAT MODEL We consider an adversary who aims to decrease the efﬁcacy of a victim NMT system. The attacker perturbs a benign sentence with unnoticeable perturbations to craft adversarial examples and feeds adversarial examples to the victim NMT. The perturbed adversarial examples will consume more computational resources of the victim NMT systems, thus impairing the translation service or make the service unavailable. Adversary’s Capabilities. The attacker is able to modify the victim’s input samples to apply the perturbations, e.g.,, By publishing some documents contain adversarial examples on the Internet. We follow existing work (Cheng et al. , 2020; Belinkov & Bisk, 2017; Li et al. , 2018; Jin et al. , 2020) and consider two types of perturbations: ( i) token-level perturbation and (ii) character-level perturbation. To ensure imperceptibility, we limit the perturbation size that the adversary can perturb the benign inputs. In Section 4, we evaluate how different will affect the effectiveness of proposed attack. Adversary’s Knowledge. To assess the security vulnerability of existing NMT systems, we study white-box scenarios, i.e., the attackers know the victim NMT systems architecture, parameters and the tokenizer that tokenize the word. In Section 4. 4, we study more practical black-box scenarios,
3


i.e.,, the attacker leverage the transferability to generate adversarial examples and attack a victim NMT system without any prior knowledge about the victim. Adversary’s Goals. The adversary’s goal is to decrease the efﬁcacy of a victim NMT system. As we discussed in Section 2, NMT system’s efﬁcacy is related to its’ output length. A longer output length indicates the NMT system costs more computational resources and becomes less efﬁcacy. Thus, the adversary can achieve their goal through increasing the NMT system’s output length.
∆ = argmaxδ Len(F (x + δ))
s.t. ||δ|| ≤
(1)
Finally, we formulate our problem of generating efﬁciency adversarial examples as an optimization problem. As shown in equation 1, where x is the benign input, F is the target NMT system, is the maximum adversarial perturbation, and Len(·) measure the length of a output sequence. Our attack TranSlowDown tries to search a perturbation ∆ that maximize the output length ( decrease target NMT system F efﬁency) while smaller than the allowed perturbation ( unnoticeable).
3. 2 TR A NSL O WDO W N ATTACK
Our attack algorithm is shown in Algo rithm 1, which iteratively performs the following three main steps: (i) ﬁnd important tokens, (ii) generate possible perturbation, and (iii) select optimal perturbation, until the generated adversarial examples reach
Algorithm 1 TranSlowDown Attack Input: Benign input x, victim NMT system F(·), maximum perturbation Output: Adversarial examples x
the maximum perturbation.
1: x ⇐ x Initialize x with x
Find Important Tokens (line 3 to 6):
2: while ||x − x|| ≤ do
Given a benign input x = [tk1, · · · , tkm], the ﬁrst step is to ﬁnd each tokens importance to NMT systems’ efﬁciency. As we discussed in Section 2, NMT systems’ efﬁciency is related to its’ output length, and the output length is determined by the probability of EOS tokens. Then our objective is to decrease the probability of the EOS token to reduce NMT’s efﬁ 3: for each tki ∈ x do
4:
Compute gi according to equation 2
5: end for
6: TK ⇐ Sort(tk1, tk2, · · · , tkm) according to gi
7: L = GeneratePerturbation(TK, x , F(·))
8: [tk, ∆] = SelectBestPerturbation(L, x , F(·))
9: x ⇐ replace tk with ∆ in x
10: end while
11: return x
ciency. Formally, let NMT system’s out put probability be a sequence of vectors,
i.e., F (x) = [p1, p2, · · · , pn]. Then the
probability
of
EOS
tokens
are
[p
eos 1
,
peos 2
,
·
·
·
,
peos n
]
.
We
seek
to
ﬁnd
the
importance
of
each
token
tki in x to this probability sequence. We also observe that the output token sequence will affect
EOS’s probability. Thus, we deﬁne the importance score of token tki as gi, shown in equation 2.
oi = argmax(pi)
1 f (x) = n
n (pieos + pioi ) i
∂f (x) j gi = ∂tkij (2)
Where [o1, o2, · · · , on] is the current output token, f (x) is the probability we seek to minimize, tkij is the jth dimension of tk’s embeddings, and gi is the derivative of f (x) to ith token’s embedding.
Adversarial Perturbation Generation (line 7): After identifying important tokens, the next step is to mutate the important token with unnoticeable perturbations. In this step, we get a set of perturbation candidate L after we perturbing the most important tokens in the original input. Following existing work, we consider two kinds of perturbations, i.e.,, token-level perturbation and characterlevel perturbation. Table 1 shows some example of token-level and characterlevel perturbation under different perturbation size , we color the perturbation with color red.
∂f (x) j Isrc,tgt = (E(tgt) − E(src)) × ∂tkij
δ = argmaxtgt Itk,tgt;
(3)
4


For token-level perturbation, we consider replacing the original token tk with another token δ. To compute the target token δ, we deﬁne token replace increment Isrc, tgt to measure the efﬁciency degradation of replacing token src to tgt. As shown in equation 3, E(·) is the function to get corresponding token’s embedding, E(tgt) − E(src) is the vector increment in embedding space. Because ∂tkj ∂f(x) indicate the sensitive of output length to each embedding dimension, Isrc,tgt repre- i sent the total beneﬁts of replacing token src with tgt. We search the target token δ in the vocabulary to maximize the token replace increment with the source token tk.
For character-level perturbation, we consider character insert perturbation. Speciﬁcally, we insert one character c into token tk to get an Table 1: Examples of token-level and characterlevel perturbation under different size
other token δ. The character-inset perturba- Original
Do you know who Rie Miyazawa is?
tion is common in the real world when typing quickly and is unnoticeable without careful examination. Because character insertion is likely to result in outof-vocabulary (OOV), thus it
Token-Level
1 Do I know who Rie Miyazawa is? 2 Do I know who Hill Miyazawa is? 3 How I know who Hill Miyazawa is?
1 Do you know who Rie Miya-zawa is? Character-Level 2 Do you know whoo Rie Miya-zawa is?
is challenging to compute the token replace in 3 Do you knoiw whoo Rie Miya-zawa is?
crement as token-level. Instead, we enumerate
possible δ after character insertion to get candidate set L. Speciﬁcally, we consider all letters and
digits as the possible character c because humans can type these tokens through the keyboard, and we
consider all positions the possible insertion position. Then for token tk, which contains l characters,
there are (l + 1) × ||C|| perturbation candidates, where ||C|| is the size of all possible characters.
Select Best Perturbation (line 8 to 9): After collecting candidate perturbations L, we select an optimal perturbation from the collected candidate sets. As our objective is searching adversarial perturbation that will produce longer output length, thus, we try all adversarial perturbation and select the optimal perturbation that produce the maximum output length.
4 EVALUATION 4. 1 EXPERIMENTAL SETUP Models and Datasets. As shown in Table 2, we consider the following three public NMT systems as our victim models: Google’s T5 (Raffel et al. , 2019), Facebook’s Fairseq Transformer (Ng et al. , 2019), and Helsinki-NLP’s H-NLP Translator (Jo¨rg et al. , 2020). For each model, we choose a speciﬁc testing dataset: (i). T5 is released by Google, it’s ﬁrst pre-trained with multiple language problems, then ﬁne-tuned on the English-German translation task. We apply English sentences from dataset ZH19 as benign examples to generate adversarial examples; (ii). Fairseq is one of the NMT models that Facebook FAIR submitted to the WMT19 shared news translation task, and it’s based on the FFN transformer architecture ( Vaswani et al. , 2017). We select Fairseq’s en-de model as our victim model, which is designed for the English-German translation task. We apply WMT19 test data as benign examples to generate adversarial examples; (iii). H-NLP is a seq2seq translation model, the source language is English and the target language is Chinese. We apply the sentences from Tatoeba-test. eng. zho as benign examples to generate adversarial samples.
Model T5 FAIR H-NLP
Table 2: The victim models in our experiments
Vocab Size Source Target URL
ZH19 WMT19 Tatoeba
En
De https://huggingface. co/t5-small
En
De https://huggingface. co/facebook/wmt19-en-de
En
Zh https://huggingface. co/Helsinki-NLP/opus-mt-en-de
Metrics. We apply ﬂoating-point operations (FLOPs) and response latency to measure victim NMT systems’ efﬁciency. FLOPs is a hardware-independent metric and is widely used to measure DNNs’ computational complexity. Higher FLOPs mean that the DNN requires more computations to handle an input, which represents less efﬁciency (Howard et al. , 2017; Zhang et al. , 2018). Response latency is a hardware-dependent metric, which can measure the victim model’s efﬁciency on benign and adversarial examples. High response latency indicates that the victim NMT system needs to spend
5


CDF PDF
Benign
Baseline
Ours
0. 15
0. 03
8
0. 10 0. 02 6
4
0. 05
0. 01
2
0
0
0
1. 0
1. 0
1. 0
0. 8
0. 8
0. 8
0. 6
0. 6
0. 6
0. 4
0. 4
0. 4
0. 2
0. 2
0. 2
0
0
0
0 20 40 60 80 100 120 140 160 180 200
0 20 40 60 80 100 120 140 160 180 200 220
0
1
2
3
4
5
FLOPs
Intel V5 CPU Latency
1080 Ti GPU Latency
Figure 2: The distribution of FLOPs and latency before and after tokenlevel attacks
more computational resources. The higher the response latency, the worse the realtime translation quality. We measure the latency on two hardware platforms: Intel Xeon E5-2660v3 CPU and Nvidia 1080Ti GPU.
To evaluate the effectiveness of TranSlowDown, we measure the FLOPs and latency on benign and adversarial examples respectively. Speciﬁcally, we ﬁrst compute the probability density function ( PDF) and cumulative distribution function (CDF) of FLOPs and latency. Then, we deﬁne two metrics: I-FLOPs and I-Latency, to evaluate how the adversarial samples affect the victim NMT systems. The formal deﬁnition of I-FLOPs and I-Latency shows in equation 4, where x denotes the benign example and x represents the adversarial example after perturbing x, FLOPs( ·) and Latency(·) are the functions to calculate the average FLOPs and latency per input character. Higher I-FLOPs and I-Latency indicate a more severe slowdown caused by the adversarial example.
FLOPs(x ) − FLOPs(x)
I-FLOPs =
× 100%
FLOPs(x)
Latency(x ) − Latency(x)
I-Latency =
× 100%
Latency(x)
(4)
Comparison Baseline. To the best of our knowledge, we are the ﬁrst to study the attack efﬁciency of NMT systems, therefore no existing efﬁciency attack framework can be applied as the baseline. To show existing accuracy-based attacks can not be applied for evaluating NMT’s efﬁcient robustness, we compare TranSlowDown against two accuracy-based attacks. We choose Seq2Sick (Cheng et al. , 2020) as the token-level baseline. Seq2Sick can replace the tokens in benign inputs to produce adversarial translation outputs that are entirely different from benign outputs. Because Seq2Sick only works under token-level, we choose SyntheticError ( Belinkov & Bisk, 2017) as the characterlevel baseline. SyntheticError minimizes the NMT system’s accuracy ( BLUE score) by introducing synthetic noise. Speciﬁcally, SyntheticError introduces four characterlevel perturbation: Swap, Middle Random, Fully Random, and Keyboard Typo to perturb benign examples to decrease the NMT system’s BLUE scores.
4. 2 TOKEN-LEVEL PERTURBATION RESULTS Effectiveness of Tokenlevel Attack. Figure 2 shows the distribution of H-NLP’s efﬁciency metrics under tokenlevel (more results for T5 and Fairseq can be found in Appendix A.2). The ﬁrst and second rows show the PDF and CDF 1 results respectively. In each plot, the blue area denotes the distribution of benign examples, green and red areas represent the distribution of adversarial examples generated from the comparison baseline Seq2Sick and TranSlowDown respectively. It can be clearly observed from Figure 2 that adversarial examples generated by TranSlowDown signiﬁcantly change the distribution of the victim NMT system’s FLOPs and latency. In contrast, the adversarial examples from baseline have very little effect on NMT’s efﬁciency. The results for the other two NMT systems in the Appendix show similar trends with Figure 2. From the results, we conclude that: existing accuracybased attacks can not be applied to evaluate the efﬁciency robustness of NMT systems. In contrast, our proposed attack, TranSlowDown, effectively generates 1For better presentation, we plot the CDF from one to zero.
6


CDF PDF
Benign
Baseline
Ours
0. 15
20
0. 03
0. 10 0. 02 10
0. 05
0. 01
0
0
0
1. 0
1. 0
1. 0
0. 8
0. 8
0. 8
0. 6
0. 6
0. 6
0. 4
0. 4
0. 4
0. 2
0. 2
0. 2
0
0
0
0 20 40 60 80 100 120 140 160 180 200
0 20 40 60 80 100 120 140 160
0
1
2
3
FLOPs
Intel V5 CPU Latency
1080 Ti GPU Latency
Figure 3: The distribution of FLOPs and latency before and after characterlevel attacks
adversarial examples to slow down NMT systems. Thus, TranSlowDown is effective in evaluating the efﬁciency robustness of NMT systems.
NMT H-NLP FairSeq T5
Table 3: The severity of token-level adversarial attacks
I-FLOPs Perturbation Baseline Ours
I-Latency (CPU) Baseline Ours
I-Latency (GPU) Baseline Ours
1
4. 18 1318. 72 4. 51 1485. 64 2. 55 1269. 53
2
6. 03 2131. 92 6. 33 2403. 21 3. 05 2054. 83
3
12. 94 2336. 88 13. 83 2636. 51 11. 10 2296. 09
4
17. 57 2360. 94 19. 40 2664. 46 15. 68 2314. 23
5
23. 56 2366. 09 25. 38 2669. 82 23. 89 2321. 93
1
-0. 05 23. 46
0. 15
24. 85
3. 50
29. 62
2
-0. 19 36. 80
0. 61
38. 97
1. 17
40. 59
3
-2. 22 47. 27
-0. 73 51. 21
-1. 40 54. 44
4
-5. 78 57. 60
-4. 17 63. 35
-4. 09 63. 57
5
-8. 20 80. 75
-6. 85 89. 73
-1. 76 85. 74
1
10. 09 335. 41
9. 66 383. 38
7. 87 352. 88
2
6. 80 343. 69
5. 44 393. 20
4. 06 362. 45
3
1. 47 343. 69
1. 64 393. 20
-1. 54 362. 45
4
-11. 66 343. 69 -9. 62 393. 20 -8. 28 362. 45
5
-25. 18 343. 69 -24. 29 393. 20 -8. 59 362. 45
Severity of Token-level Attack. To quantify the severity of the proposed efﬁciency attack, we measure I-FLOPs and I-Latency under different perturbation sizes. From the results in Table 3, we have the following observations: (i) For all experimental subjects, the adversarial examples generated by TranSlowDown slow down the victim NMT systems by a large margin compared to the baseline method. For H-NLP, TranSlowDown adversarial examples increase the FLOPs, CPU latency, GPU latency up to 2366. 09%, 2669. 82%, 2321. 93% respectively. (ii) As perturbation size increases, the adversarial examples generated by TranSlowDown can make the victim NMT system slower. However, this observation does not hold for the baseline method. This is because the baseline method is designed to decrease victim NMT’s accuracy, increasing adversarial perturbation may decrease NMT accuracy but does not imply to consume more computational resources. (iii) The adversarial examples generated by the baseline method can not always ensure increasing FLOPs and latency, while TranSlowDown adversarial examples increase FLOPs and latency on all settings. This result is consistent with the second observation and indicates that the baseline method is not suitable to evaluate NMT’s computational consumption robustness.
4. 3 CHARACTER PERTURBATION Effectiveness of Characterlevel Attack. Figure 3 shows the distribution of H-NLP’s efﬁciency metrics under characterlevel (more results for T5 and Fairseq can be found in Appendix A.2). For characterlevel attack, compared with baseline, adversarial examples generated by TranSlowDown
7


signiﬁcantly change the distribution of the victim NMT system’s FLOPs and latency as well. The results for the other two NMT systems in the Appendix show similar trends with Figure 3. From the results, we obtain a similar conclusion with the tokenlevel attack, i.e.,, our proposed attack, TranSlowDown, effectively generates adversarial examples to slow down NMT systems. In contrast, the accuracy-based attack can not achieve this goal.
NMT H-NLP FairSeq T5
Table 4: The severity of character-level adversarial attacks
I-FLOPs Perturbation Baseline Ours
I-Latency (CPU) Baseline Ours
I-Latency (GPU) Baseline Ours
1
20. 09 389. 36 21. 27 431. 72 11. 84 368. 08
2
20. 09 879. 16 21. 29 978. 47 12. 07 840. 47
3
20. 09 1102. 86 21. 29 1232. 71 12. 07 1056. 88
4
20. 09 1189. 48 21. 29 1328. 29 12. 07 1136. 70
5
20. 09 1224. 91 21. 29 1366. 22 12. 07 1174. 57
1
0. 28
36. 23
0. 71
38. 59
3. 15
40. 13
2
0. 28
87. 92
0. 71
97. 37
3. 17
94. 31
3
0. 28 145. 60
0. 71 164. 90
3. 17 155. 43
4
0. 28 190. 43
0. 71 217. 94
3. 17 204. 02
5
0. 28 223. 07
0. 71 255. 42
3. 17 235. 93
1
5. 60 217. 46
6. 39 249. 44
5. 71 229. 70
2
5. 56 249. 88
6. 37 286. 81
5. 69 258. 35
3
5. 56 267. 58
6. 37 307. 57
5. 69 273. 41
4
5. 56 276. 33
6. 37 318. 40
5. 69 283. 37
5
5. 56 280. 10
6. 37 323. 67
5. 69 286. 84
Severity of Character-level Attack. Similar to Section 4. 2, Table 4 presents the severity of character-level adversarial examples. The results show consistency with the token-level attack. For the character-level attack, the adversarial examples generated by TranSlowDown also signiﬁcantly slow down the victim NMT systems compared to the baseline method. For H-NLP, TranSlowDown adversarial examples increase the FLOPs, CPU latency, GPU latency up to 1224. 91%, 2669. 82%, 1174. 57% respectively. As perturbation size increases, the adversarial examples generated by TranSlowDown can make the victim NMT system slower, which is not reﬂected in the baseline results. This is consistent with the token-level attack.
4. 4 THE TRANSFERABILITY OF ADVERSARIAL EXAMPLES In this section, we study the transferability of efﬁcacy adversarial examples. Even though whitebox attacks are important to expose the vulnerability, black-box attacks are more practical in the real world because they require less information about the NMT systems. We investigate whether TranSlowDown is transferable under blackbox settings.
Table 5: The maximum I-FLOPs of blackbox token-level attack
Source Target
1
2
3
4
5
FairSeq 185. 71 57. 14 68. 75 57. 14 42. 86 H-NLP T5 1600.00 1600.00 1600.00 1600.00 1600.00
H-NLP 6566. 67 6566. 67 6566. 67 6566. 67 6566. 67 FairSeq T5 1600.00 1600.00 1600.00 1600.00 1600.00
H-NLP 3733. 33 3733. 33 3733. 33 3733. 33 3733. 33 T5 FairSeq 858. 33 1816. 67 1816. 67 945. 45 945. 45
Speciﬁcally, we treat one NMT system as a target and apply another NMT system as the source to generate adversarial examples. We then feed the adversarial examples to the target NMT systems and measure the maximum I-FLOPs. Maximum I-FLOPs indicate the efﬁciency degradation under the worst scenario, which is important to measure the vulnerability of NMT systems. Notice the source and the target NMT systems in our experiment adopts different model architectures and are trained with different datasets. Thus, if the adversarial examples can increase the FLOPs of the target NMT system, it proves that transferability exists in our attack. The results for token-level attacks are shown in Table 5 ( more results in Appendix A.3). From the results, we observe that for
8


all experimental settings, TranSlowDown generates adversarial examples that increase the target NMT system’s computational FLOPs to a large extend. The results imply that under the worst scenarios, the attackers can generate efﬁcient adversarial examples even without prior knowledge about the victim NMT systems. 5 REAL WORLD CASE STUDY We conduct a case study to evaluate TranSlowDown’s ability to attack real world mobile devices’ battery power.
Experiment Settings. We select Google’s T5 as our victim NMT model. We ﬁrst de- Table 6: Sentences for energy attack on mobile devices
ploy the model on Samsung Galaxy S9+, which has 6GB RAM and a battery capacity of 3500 mAh. After that, we select one sentence from the dataset ZH19 as our testing example; We then apply
Benign Adversarial
Death comes often to the soldiers and marines who are ﬁghting in anbar province, which is roughly the size of louisiana and is the most intractable region in iraq. Death comes often to the soldiers and marines who are ﬁghting in anbar province, which is roughly the (size of of louisiana and is the most intractable region in iraq.
TranSlowDown to perturb the benign
example with character-level perturbation and obtain the corresponding adversarial example. The
benign sentence and the adversarial sentence are shown in Table 6 ( More results in Appendix sec tion A.4), we color the perturbation with the color red. Notice the adversarial example insert only
one character in the benign sentence. This one-character perturbation is very common in the real
world because of the user’s typos. Finally, we feed the benign and adversarial examples to the
deployed NMT system and measure the NMT’s energy consumption of translating benign and ad versarial examples.
Experiment Results. The mobile’s battery power 100
change is shown in Figure 4. The red line is for ad versarial example, and the blue line is for benign ex90 ample. The results show that the adversarial exam Energy (%)
ple increases the mobile device’s battery power sig niﬁcantly more signiﬁcant than the benign example. 80
Speciﬁcally, after 300 iterations, the adversarial ex ample consumes 30% of the battery power, while the
Adversarail
70
Benign
benign example only consumes less than 1%. The
0
results show the vulnerability of the efﬁciency attack
100
200
300
Iteration Number
for IoT and mobile devices. Recall that the adversarial example used in our experiment only inserts one character in the benign sentence. This minimal perturbation can result from benign user typos instead
Figure 4: Remaining battery power of the mobile device after T5 translating benign and adversarial sentence
of from the adversary. Thus, the results suggest the criticality and the necessity of increasing NMT
systems’ efﬁciency robustness.
6 DISCUSSION Although there are many accuracy-based defense mechanisms to protect DNNs. However, accuracyoriented defense mechanisms are not applicable for protecting NMT systems’ efﬁciency robustness because of two reasons: (i) Many existing accuracy defense mechanisms require running the model with the input and use the hidden states of the model to make decision. However, running the input would defeat the purpose of an energy defense as the computation resource is already consumed. (ii) Our results in section 4 show that accuracybased adversarial examples and efﬁciency-based adversarial examples belong to different distribution, thus, running accuracy-based detector can not detect efﬁciencybased adversarial examples successfully.
7 CONCLUSIONS In this work, we study the robustness of NMT systems against adversarial efﬁciency attacks. Specifically, we propose TranSlowDown, an attack that introduces imperceptible adversarial perturbations to benign inputs to increase NMT’s computational complexity. We evaluate TranSlowDown on three public available NMT systems, the results show TranSlowDown generates adversarial examples decrease NMT systems’ efﬁciency. Our study suggests that efﬁciency attacks are a vulnerable, yet under-appreciated, threat against NMT systems.
9

